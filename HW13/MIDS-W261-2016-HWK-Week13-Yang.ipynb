{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "####Assignement:  week \\#13\n",
    "####[Lei Yang](mailto:leiyang@berkeley.edu) | [Michael Kennedy](mailto:mkennedy@ischool.berkeley.edu) | [Natarajan Krishnaswami](mailto:natarajan@krishnaswami.org)\n",
    "####Due: 2016-04-22, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.9 (default, Dec 15 2014 10:37:34)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/leiyang/Downloads/spark-1.6.1-bin-hadoop2.6/'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###HW 13.1: Spark implementation of basic PageRank\n",
    "\n",
    "Write a basic Spark implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input.\n",
    "Make sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), \n",
    "and further, distributes the mass of dangling nodes with each iteration\n",
    "so that the output of each iteration is correctly normalized (sums to 1).\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,\n",
    "chooses the next page to which it will move by clicking at random, with probability d,\n",
    "one of the hyperlinks in the current page. This probability is represented by a so-called\n",
    "‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer\n",
    "jumps to any web page in the network. If a page is a dangling end, meaning it has no\n",
    "outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform\n",
    "distribution and “teleports” to that page]\n",
    "\n",
    "In your Spark solution, please use broadcast variables and caching to make sure your code is as efficient as possible.\n",
    "\n",
    "\n",
    "As you build your code, use the test data\n",
    "\n",
    "s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck\n",
    "your work with the true result, displayed in the first image\n",
    "in the Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "\n",
    "A,0.033\n",
    "B,0.384\n",
    "C,0.343\n",
    "D,0.039\n",
    "E,0.081\n",
    "F,0.039\n",
    "G,0.016\n",
    "H,0.016\n",
    "I,0.016\n",
    "J,0.016\n",
    "K,0.016\n",
    "\n",
    "Run this experiment locally first. Report the local configuration that you used and how long in minutes and seconds it takes to complete your job.\n",
    "\n",
    "Repeat this experiment on AWS. Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete your job. (in your notebook, cat the cluster config file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "def initialize(line):\n",
    "    # parse line\n",
    "    nid, adj = line.strip().split('\\t', 1)\n",
    "    exec 'adj = %s' %adj\n",
    "    # initialize node struct\n",
    "    node = {'a':adj.keys(), 'p':0}\n",
    "    rankMass = 1.0/len(adj)\n",
    "    # emit pageRank mass and node\n",
    "    return [(m, rankMass) for m in node['a']] + [(nid.strip('\"'), node)]\n",
    "\n",
    "def accumulateMass(a, b):\n",
    "    if isinstance(a, float) and isinstance(b, float):\n",
    "        return a+b\n",
    "    if isinstance(a, float) and not isinstance(b, float):\n",
    "        b['p'] += a\n",
    "        return b\n",
    "    else:\n",
    "        a['p'] += b\n",
    "        return a\n",
    "\n",
    "def getDangling(node):\n",
    "    global nDangling\n",
    "    if isinstance(node[1], float):\n",
    "        nDangling += 1\n",
    "        return (node[0], {'a':[], 'p':node[1]})\n",
    "    else:\n",
    "        return node\n",
    "\n",
    "def redistributeMass(node):\n",
    "    node[1]['p'] = (p_dangling.value+node[1]['p'])*damping + alpha\n",
    "    return node\n",
    "\n",
    "def distributeMass(node):\n",
    "    global lossMass\n",
    "    mass, adj = node[1]['p'], node[1]['a']\n",
    "    node[1]['p'] = 0\n",
    "    if len(adj) == 0:\n",
    "        lossMass += mass\n",
    "        return [node]\n",
    "    else:\n",
    "        rankMass = mass/len(adj)\n",
    "        return [(x, rankMass) for x in adj]+[node]\n",
    "\n",
    "def getIndex(line):\n",
    "    elem = line.strip().split('\\t')\n",
    "    return (elem[1], elem[0])\n",
    "\n",
    "def logTime():\n",
    "    return str(datetime.now())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-04-15 22:52:21.595538: start PageRank initialization ...\n",
      "2016-04-15 22:52:22.789142: initialization completed, dangling node(s): 1, total nodes: 11\n",
      "2016-04-15 22:52:22.789784: running iteration 2 ...\n",
      "2016-04-15 22:52:22.893942: redistributing loss mass: 0.6523\n",
      "2016-04-15 22:52:22.900249: running iteration 3 ...\n",
      "2016-04-15 22:52:23.016227: redistributing loss mass: 0.4174\n",
      "2016-04-15 22:52:23.021943: running iteration 4 ...\n",
      "2016-04-15 22:52:23.103576: redistributing loss mass: 0.7042\n",
      "2016-04-15 22:52:23.108007: running iteration 5 ...\n",
      "2016-04-15 22:52:23.193200: redistributing loss mass: 0.4136\n",
      "2016-04-15 22:52:23.197213: running iteration 6 ...\n",
      "2016-04-15 22:52:23.274630: redistributing loss mass: 0.4254\n",
      "2016-04-15 22:52:23.278632: running iteration 7 ...\n",
      "2016-04-15 22:52:23.352806: redistributing loss mass: 0.3753\n",
      "2016-04-15 22:52:23.356641: running iteration 8 ...\n",
      "2016-04-15 22:52:23.430620: redistributing loss mass: 0.3812\n",
      "2016-04-15 22:52:23.434674: running iteration 9 ...\n",
      "2016-04-15 22:52:23.531019: redistributing loss mass: 0.3659\n",
      "2016-04-15 22:52:23.536714: running iteration 10 ...\n",
      "2016-04-15 22:52:23.635844: redistributing loss mass: 0.3660\n",
      "2016-04-15 22:52:23.679582: normalized weight of the graph: 1.0000\n",
      "2016-04-15 22:52:23.679781: PageRanking completed in 0.03 minutes.\n",
      "2016-04-15 22:52:24.634718: results saved, job completed!\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "# load the graph\n",
    "graph_file = sc.textFile('hdfs://localhost:9000/user/leiyang/PageRank-test.txt')\n",
    "index_file = sc.textFile('file:///Users/leiyang/Downloads/toy_index.txt')\n",
    "\n",
    "# initialize variables\n",
    "nDangling = sc.accumulator(0)\n",
    "lossMass = sc.accumulator(0.0)\n",
    "damping = 0.85\n",
    "alpha = 1 - damping\n",
    "nTop, nIter = 100, 10\n",
    "start = time()\n",
    "print '%s: start PageRank initialization ...' %(logTime())\n",
    "graph = graph_file.flatMap(initialize).reduceByKey(accumulateMass).map(getDangling).cache()\n",
    "# get graph size\n",
    "G = graph.count()\n",
    "# broadcast dangling mass for redistribution\n",
    "p_dangling = sc.broadcast(1.0*nDangling.value/G)\n",
    "graph = graph.map(redistributeMass)\n",
    "print '%s: initialization completed, dangling node(s): %d, total nodes: %d' %(logTime(), nDangling.value, G)\n",
    "\n",
    "for i in range(nIter-1):\n",
    "    print '%s: running iteration %d ...' %(logTime(), i+2)\n",
    "    lossMass.value = 0.0\n",
    "    graph = graph.flatMap(distributeMass).reduceByKey(accumulateMass).cache() #checkpoint()?\n",
    "    # need to call an action here in order to have loss mass\n",
    "    graph.count()\n",
    "    print '%s: redistributing loss mass: %.4f' %(logTime(), lossMass.value)\n",
    "    p_dangling = sc.broadcast(lossMass.value/G)\n",
    "    graph = graph.map(redistributeMass)\n",
    "\n",
    "totalMass = graph.aggregate(0, (lambda x, y: y[1]['p'] + x), (lambda x, y: x+y))\n",
    "print '%s: normalized weight of the graph: %.4f' %(logTime(), totalMass/G)\n",
    "print '%s: PageRanking completed in %.2f minutes.' %(logTime(), (time()-start)/60.0)\n",
    "# get the page name by join\n",
    "topPages = graph.map(lambda n:(n[0],n[1]['p']/G)).sortBy(lambda n: n[1], ascending=False).take(nTop)\n",
    "rankList = index_file.map(getIndex).join(sc.parallelize(topPages)).map(lambda l: l[1])\n",
    "# save final rank list\n",
    "rankList.sortBy(lambda n: n[1], ascending=False).saveAsTextFile('pageRank')\n",
    "print '%s: results saved, job completed!' %logTime()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u\"'Node_b\", 0.3632359489889102)\r\n",
      "(u\"'Node_c\", 0.36288372803871793)\r\n",
      "(u\"'Node_e\", 0.08114525762548769)\r\n",
      "(u\"'Node_d\", 0.03938466342002967)\r\n",
      "(u\"'Node_f\", 0.03938466342002967)\r\n",
      "(u\"'Node_a\", 0.03293010178620472)\r\n",
      "(u\"'Node_h\", 0.016207127344124005)\r\n",
      "(u\"'Node_j\", 0.016207127344124005)\r\n",
      "(u\"'Node_g\", 0.016207127344124005)\r\n",
      "(u\"'Node_i\", 0.016207127344124005)\r\n",
      "(u\"'Node_k\", 0.016207127344124005)\r\n"
     ]
    }
   ],
   "source": [
    "!cat pageRank/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 13.2: Applying PageRank to the Wikipedia hyperlinks network\n",
    "\n",
    "Run your Spark PageRank implementation on the Wikipedia dataset for 10 iterations,\n",
    "and display the top 100 ranked nodes (with alpha = 0.85).\n",
    "\n",
    "Run your PageRank implementation on the Wikipedia dataset for 50 iterations,\n",
    "and display the top 100 ranked nodes (with teleportation factor of 0.15). \n",
    "Plot the pagerank values for the top 100 pages resulting from the 50 iterations run. Then plot the pagerank values for the same 100 pages that resulted from the 10 iterations run.  Comment on your findings.  Have the top 100 ranked pages changed? Have the pagerank values changed? Explain.\n",
    "\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete your job.\n",
    "\n",
    "NOTE: ====  English Wikipedia hyperlink network.data ====\n",
    "The dataset is available via Dropbox at:\n",
    "\n",
    "https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0\n",
    "\n",
    "on S3 at  s3://ucb-mids-mls-networks/wikipedia/\n",
    "-- s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt # Graph\n",
    "-- s3://ucb-mids-mls-networks/wikipedia/indices.txt               # Page titles and page Ids\n",
    "\n",
    "The dataset is built from the Sept. 2015 XML snapshot of English Wikipedia.\n",
    "For this directed network, a link between articles: \n",
    "\n",
    "A -> B\n",
    "\n",
    "is defined by the existence of a hyperlink in A pointing to B.\n",
    "This network also exists in the indexed format:\n",
    "\n",
    "Data: s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt\n",
    "Data: s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-in.txt\n",
    "Data: s3://ucb-mids-mls-networks/wikipedia/indices.txt\n",
    "\n",
    "but has an index with more detailed data:\n",
    "\n",
    "(article name) \\t (index) \\t (in degree) \\t (out degree)\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values .\n",
    "Here, a weight indicates the number of time a page links to another.\n",
    "However, for the sake of this assignment, treat this an unweighted network,\n",
    "and set all weights to 1 upon data input.\n",
    "\n",
    "\n",
    "###Submit job to the cluster\n",
    "- ssh i the master node of EMR cluster\n",
    "- do data preparation\n",
    "- submit job to spark\n",
    "\n",
    "###Cluster configuration: 1 m1.large master node + 9 m1.large task nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "/usr/bin/spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode client \\\n",
    "--name LeiPageRankToy \\\n",
    "--py-files PageRank.py \\\n",
    "--executor-memory '4600m' \\\n",
    "--executor-cores 2 \\\n",
    "--driver-memory '4600m' \\\n",
    "--num-executors 11 \\\n",
    "PageRankDriver.py > wiki_10_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###10 Iterations [log](https://raw.githubusercontent.com/leiyang-mids/w261_project/master/HW13/spark_log_10_iterations?token=AL1BFhvNgpQxAyXpSJR1HHtWVHm3pDwJks5XJE0kwA%3D%3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-04-22 21:47:50.407410: start PageRank initialization ...\r\n",
      "2016-04-22 21:51:14.131401: initialization completed, dangling node(s): 9410987, total nodes: 15192277\r\n",
      "2016-04-22 21:51:14.131474: running iteration 2 ...\r\n",
      "2016-04-22 21:54:16.556416: redistributing loss mass: 7608969.0130\r\n",
      "2016-04-22 21:54:16.564424: running iteration 3 ...\r\n",
      "2016-04-22 21:57:10.348773: redistributing loss mass: 7103036.3037\r\n",
      "2016-04-22 21:57:10.356219: running iteration 4 ...\r\n",
      "2016-04-22 21:59:57.663450: redistributing loss mass: 6940792.1449\r\n",
      "2016-04-22 21:59:57.720320: running iteration 5 ...\r\n",
      "2016-04-22 22:02:49.159656: redistributing loss mass: 6884560.5231\r\n",
      "2016-04-22 22:02:49.166445: running iteration 6 ...\r\n",
      "2016-04-22 22:05:39.179975: redistributing loss mass: 6863177.9617\r\n",
      "2016-04-22 22:05:39.186458: running iteration 7 ...\r\n",
      "2016-04-22 22:08:22.604490: redistributing loss mass: 6854533.8830\r\n",
      "2016-04-22 22:08:22.610610: running iteration 8 ...\r\n",
      "2016-04-22 22:11:08.030830: redistributing loss mass: 6850834.1086\r\n",
      "2016-04-22 22:11:08.037460: running iteration 9 ...\r\n",
      "2016-04-22 22:13:54.898048: redistributing loss mass: 6849174.4055\r\n",
      "2016-04-22 22:13:54.904239: running iteration 10 ...\r\n",
      "2016-04-22 22:16:43.731913: redistributing loss mass: 6848394.4811\r\n",
      "2016-04-22 22:17:27.460246: normalized weight of the graph: 1.0000\r\n",
      "2016-04-22 22:17:27.460319: PageRanking completed in 29.62 minutes.\r\n",
      "2016-04-22 22:22:26.496599: results saved, job completed!\r\n"
     ]
    }
   ],
   "source": [
    "!cat pageRank_time_10_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###50 Iterations [log](https://raw.githubusercontent.com/leiyang-mids/w261_project/master/HW13/pageRank_time_50_iterations?token=AL1BFgR-jEQyDh6rJxB4Ra7r0YRhquhDks5XJE1NwA%3D%3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hadoop@ip-172-31-9-186 lei]$ cat wiki_50_log\r\n",
      "2016-04-22 22:27:22.269262: start PageRank initialization ...\r\n",
      "2016-04-22 22:30:40.932047: initialization completed, dangling node(s): 9410987, total nodes: 15192277\r\n",
      "2016-04-22 22:30:40.932248: running iteration 2 ...\r\n",
      "2016-04-22 22:33:38.183716: redistributing loss mass: 7608969.0130\r\n",
      "2016-04-22 22:33:38.191269: running iteration 3 ...\r\n",
      "2016-04-22 22:36:25.688220: redistributing loss mass: 7103036.3037\r\n",
      "2016-04-22 22:36:25.695027: running iteration 4 ...\r\n",
      "2016-04-22 22:39:13.888332: redistributing loss mass: 6940792.1449\r\n",
      "2016-04-22 22:39:13.895518: running iteration 5 ...\r\n",
      "2016-04-22 22:42:05.660363: redistributing loss mass: 6884560.5231\r\n",
      "2016-04-22 22:42:05.667313: running iteration 6 ...\r\n",
      "2016-04-22 22:44:48.044150: redistributing loss mass: 6863177.9617\r\n",
      "2016-04-22 22:44:48.050751: running iteration 7 ...\r\n",
      "2016-04-22 22:47:42.365782: redistributing loss mass: 6854533.8830\r\n",
      "2016-04-22 22:47:42.372406: running iteration 8 ...\r\n",
      "2016-04-22 22:50:25.247508: redistributing loss mass: 6850834.1086\r\n",
      "2016-04-22 22:50:25.255316: running iteration 9 ...\r\n",
      "2016-04-22 22:53:09.848617: redistributing loss mass: 6849174.4055\r\n",
      "2016-04-22 22:53:09.854675: running iteration 10 ...\r\n",
      "2016-04-22 22:55:57.404165: redistributing loss mass: 6848394.4811\r\n",
      "2016-04-22 22:55:57.411764: running iteration 11 ...\r\n",
      "2016-04-22 22:58:41.922640: redistributing loss mass: 6848013.2672\r\n",
      "2016-04-22 22:58:41.928333: running iteration 12 ...\r\n",
      "2016-04-22 23:01:32.333966: redistributing loss mass: 6847819.5157\r\n",
      "2016-04-22 23:01:32.340206: running iteration 13 ...\r\n",
      "2016-04-22 23:04:18.220337: redistributing loss mass: 6847717.6750\r\n",
      "2016-04-22 23:04:18.226064: running iteration 14 ...\r\n",
      "2016-04-22 23:07:08.545158: redistributing loss mass: 6847662.3248\r\n",
      "2016-04-22 23:07:08.551160: running iteration 15 ...\r\n",
      "2016-04-22 23:09:57.948047: redistributing loss mass: 6847631.4424\r\n",
      "2016-04-22 23:09:57.953785: running iteration 16 ...\r\n",
      "2016-04-22 23:12:41.704867: redistributing loss mass: 6847613.7325\r\n",
      "2016-04-22 23:12:41.711388: running iteration 17 ...\r\n",
      "2016-04-22 23:15:23.701817: redistributing loss mass: 6847603.3743\r\n",
      "2016-04-22 23:15:23.707803: running iteration 18 ...\r\n",
      "2016-04-22 23:18:11.871462: redistributing loss mass: 6847597.1814\r\n",
      "2016-04-22 23:18:11.877195: running iteration 19 ...\r\n",
      "2016-04-22 23:20:58.774307: redistributing loss mass: 6847593.4216\r\n",
      "2016-04-22 23:20:58.781808: running iteration 20 ...\r\n",
      "2016-04-22 23:23:42.773786: redistributing loss mass: 6847591.0941\r\n",
      "2016-04-22 23:23:42.779676: running iteration 21 ...\r\n",
      "2016-04-22 23:26:31.514418: redistributing loss mass: 6847589.6330\r\n",
      "2016-04-22 23:26:31.520147: running iteration 22 ...\r\n",
      "2016-04-22 23:29:15.093137: redistributing loss mass: 6847588.6974\r\n",
      "2016-04-22 23:29:15.099334: running iteration 23 ...\r\n",
      "2016-04-22 23:32:04.386726: redistributing loss mass: 6847588.0890\r\n",
      "2016-04-22 23:32:04.394860: running iteration 24 ...\r\n",
      "2016-04-22 23:34:56.203852: redistributing loss mass: 6847587.6843\r\n",
      "2016-04-22 23:34:56.209404: running iteration 25 ...\r\n",
      "2016-04-22 23:37:45.932104: redistributing loss mass: 6847587.4101\r\n",
      "2016-04-22 23:37:45.937876: running iteration 26 ...\r\n",
      "2016-04-22 23:40:33.264800: redistributing loss mass: 6847587.2194\r\n",
      "2016-04-22 23:40:33.270828: running iteration 27 ...\r\n",
      "2016-04-22 23:43:20.503157: redistributing loss mass: 6847587.0841\r\n",
      "2016-04-22 23:43:20.508909: running iteration 28 ...\r\n",
      "2016-04-22 23:46:05.315308: redistributing loss mass: 6847586.9854\r\n",
      "2016-04-22 23:46:05.361283: running iteration 29 ...\r\n",
      "2016-04-22 23:48:51.634831: redistributing loss mass: 6847586.9119\r\n",
      "2016-04-22 23:48:51.641059: running iteration 30 ...\r\n",
      "2016-04-22 23:51:37.549179: redistributing loss mass: 6847586.8559\r\n",
      "2016-04-22 23:51:37.555017: running iteration 31 ...\r\n",
      "2016-04-22 23:54:25.350505: redistributing loss mass: 6847586.8124\r\n",
      "2016-04-22 23:54:25.356260: running iteration 32 ...\r\n",
      "2016-04-22 23:57:13.681911: redistributing loss mass: 6847586.7780\r\n",
      "2016-04-22 23:57:13.688034: running iteration 33 ...\r\n",
      "2016-04-23 00:00:00.771290: redistributing loss mass: 6847586.7505\r\n",
      "2016-04-23 00:00:00.777150: running iteration 34 ...\r\n",
      "2016-04-23 00:02:55.747675: redistributing loss mass: 6847586.7281\r\n",
      "2016-04-23 00:02:55.753448: running iteration 35 ...\r\n",
      "2016-04-23 00:05:50.139292: redistributing loss mass: 6847586.7098\r\n",
      "2016-04-23 00:05:50.145276: running iteration 36 ...\r\n",
      "2016-04-23 00:08:46.292962: redistributing loss mass: 6847586.6947\r\n",
      "2016-04-23 00:08:46.298539: running iteration 37 ...\r\n",
      "2016-04-23 00:11:32.824491: redistributing loss mass: 6847586.6821\r\n",
      "2016-04-23 00:11:32.830260: running iteration 38 ...\r\n",
      "2016-04-23 00:14:17.957812: redistributing loss mass: 6847586.6716\r\n",
      "2016-04-23 00:14:17.963311: running iteration 39 ...\r\n",
      "2016-04-23 00:17:15.913726: redistributing loss mass: 6847586.6628\r\n",
      "2016-04-23 00:17:15.919166: running iteration 40 ...\r\n",
      "2016-04-23 00:20:01.584078: redistributing loss mass: 6847586.6555\r\n",
      "2016-04-23 00:20:01.590802: running iteration 41 ...\r\n",
      "2016-04-23 00:22:51.051490: redistributing loss mass: 6847586.6493\r\n",
      "2016-04-23 00:22:51.057546: running iteration 42 ...\r\n",
      "2016-04-23 00:25:39.505687: redistributing loss mass: 6847586.6440\r\n",
      "2016-04-23 00:25:39.511355: running iteration 43 ...\r\n",
      "2016-04-23 00:28:27.865999: redistributing loss mass: 6847586.6396\r\n",
      "2016-04-23 00:28:27.871538: running iteration 44 ...\r\n",
      "2016-04-23 00:31:23.585961: redistributing loss mass: 6847586.6358\r\n",
      "2016-04-23 00:31:23.591691: running iteration 45 ...\r\n",
      "2016-04-23 00:34:09.044370: redistributing loss mass: 6847586.6327\r\n",
      "2016-04-23 00:34:09.049998: running iteration 46 ...\r\n",
      "2016-04-23 00:36:57.104713: redistributing loss mass: 6847586.6300\r\n",
      "2016-04-23 00:36:57.112818: running iteration 47 ...\r\n",
      "2016-04-23 00:39:42.006896: redistributing loss mass: 6847586.6277\r\n",
      "2016-04-23 00:39:42.012373: running iteration 48 ...\r\n",
      "2016-04-23 00:42:25.421774: redistributing loss mass: 6847586.6258\r\n",
      "2016-04-23 00:42:25.427903: running iteration 49 ...\r\n",
      "2016-04-23 00:45:16.739738: redistributing loss mass: 6847586.6242\r\n",
      "2016-04-23 00:45:16.745799: running iteration 50 ...\r\n",
      "2016-04-23 00:48:06.250673: redistributing loss mass: 6847586.6228\r\n",
      "2016-04-23 00:48:46.169341: normalized weight of the graph: 1.0000\r\n",
      "2016-04-23 00:48:46.169409: PageRanking completed in 141.40 minutes.\r\n",
      "2016-04-23 00:53:36.124725: results saved, job completed!\r\n"
     ]
    }
   ],
   "source": [
    "!cat pageRank_time_50_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 13.3: Spark GraphX versus your implementation of PageRank\n",
    "\n",
    "Run the Spark  GraphX PageRank implementation on the Wikipedia dataset for 10 iterations,\n",
    "and display the top 100 ranked nodes (with alpha = 0.85).\n",
    "\n",
    "Run your PageRank implementation on the Wikipedia dataset for 50 iterations,\n",
    "and display the top 100 ranked nodes (with teleportation factor of 0.15). \n",
    "Have the top 100 ranked pages changed? Comment on your findings. Plot both 100 curves.\n",
    "\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.\n",
    "\n",
    "Put the runtime results of HW13.2 and HW13.3 in a tabular format (with rows corresponding to implemention and columns corresponding to experiment setup (10 iterations, 50 iterations)). Discuss the run times and explaing the differences. \n",
    "\n",
    "Plot the pagerank values for the top 100 pages resulting from the 50 iterations run (using GraphX). Then plot the pagerank values for the same 100 pages that resulted from the 50 iterations run of your homegrown pagerank implemnentation.  Comment on your findings.  Have the top 100 ranked pages changed? Have the pagerank values changed? Explain.\n",
    "\n",
    "\n",
    "### Scala code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.lib._\n",
    "\n",
    "\n",
    "object WikiPageRank {\n",
    "    \n",
    "  def main(args: Array[String]) {\n",
    "    val t0 = System.nanoTime()        \n",
    "    val conf = new SparkConf().setAppName(\"WikiPageRank\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    var nIter = args(0).toInt\n",
    "\n",
    "    // Create an RDD for the edges and vertices    \n",
    "    val links = sc.textFile(\"hdfs:///user/leiyang/all-pages-indexed-out.txt\", 80).flatMap(getLinks);\n",
    "    val pages = sc.textFile(\"hdfs:///user/leiyang/indices.txt\", 16).map(getPages);\n",
    "\n",
    "    // Build the initial Graph\n",
    "    val graph = Graph(pages, links);\n",
    "    // Run pageRank\n",
    "    val rank = PageRank.run(graph, numIter=nIter).vertices.cache()\n",
    "    // Normalize the rank score\n",
    "    val total = rank.map(l=>l._2).sum()\n",
    "    val tops = rank.sortBy(l=>l._2, ascending=false).take(200).map(l => (l._1, l._2/total))\n",
    "    val ret = sc.parallelize(tops).join(pages).map(l => (l._2._2._1, l._2._1)).sortBy(l=>l._2, ascending=false).take(200)\n",
    "    val elapse = (System.nanoTime()-t0)/1000000000.0/60.0\n",
    "    // Show results\n",
    "    println(\"PageRanking finishes in \" + elapse + \" minutes!\")\n",
    "    println(ret.mkString(\"\\n\"))\n",
    "  }\n",
    "\n",
    "  def getLinks(line: String): Array[Edge[String]] = {\n",
    "      val elem = line.split(\"\\t\", 2)\n",
    "      for {n <-  elem(1).stripPrefix(\"{\").split(\",\")\n",
    "          // get Edge between id\n",
    "      }yield Edge(elem(0).toLong, n.split(\":\")(0).trim().stripPrefix(\"'\").stripSuffix(\"'\").toLong, \"\")\n",
    "  }\n",
    "\n",
    "  def getPages(line: String): (VertexId, (String, String)) = {\n",
    "      val elem = line.split(\"\\t\")\n",
    "      return (elem(1).toLong, (elem(0), \"\"))\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###10 Iterations [log](https://raw.githubusercontent.com/leiyang-mids/w261_project/master/HW13/spark_log_10_iterations_graphX?token=AL1BFpUUjZUxgTLgn3EQJfHN6QF3CNfIks5XJPISwA%3D%3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/spark-submit \\\n",
    "#--master yarn \\\n",
    "#--deploy-mode client \\\n",
    "#--class WikiPageRank \\\n",
    "#--name \"WikiPageRank\" \\\n",
    "#--executor-memory '4600m' \\\n",
    "#--num-executors 11 \\\n",
    "#--driver-memory '4600m' \\\n",
    "#target/scala-2.10/pagerank-project_2.10-1.0.jar \\\n",
    "#10 > wiki_10_log_GraphX\n",
    "#!cat top_200_after_10_iterations_graphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 13.4: Criteo Phase 2 baseline\n",
    "\n",
    "\n",
    "SPECIAL NOTE:\n",
    "Please share your findings as they become available with class via the Google Group. You will get brownie points for this.  Once results are shared please use them and build on them.\n",
    "\n",
    "The Criteo data for this challenge is located in the following S3/Dropbox buckets:\n",
    "\n",
    "- On Dropbox see:     https://www.dropbox.com/sh/dnevke9vsk6yj3p/AABoP-Kv2SRxuK8j3TtJsSv5a?dl=0\n",
    "\n",
    "- Raw Data:  (Training, Validation and Test data)\n",
    "https://console.aws.amazon.com/s3/home?region=us-west-1#&bucket=criteo-dataset&prefix=rawdata/\n",
    "\n",
    "- Hashed Data: Training, Validation and Test data in hash encoded (10,000 buckets) and sparse representation\n",
    "https://console.aws.amazon.com/s3/home?region=us-west-1#&bucket=criteo-dataset&prefix=processeddata/\n",
    "- source: https://s3-eu-west-1.amazonaws.com/criteo-labs/dac.tar.gz\n",
    "\n",
    "\n",
    "Using the training dataset, validation dataset and testing dataset in the Criteo bucket perform the following experiment:\n",
    "\n",
    "- write spark code (borrow from Phase 1 of this project) to train a logistic regression model with the following hyperparamters:\n",
    "\n",
    " - Number of buckets for hashing: 1,000\n",
    " - Logistic Regression: no regularization term\n",
    " - Logistic Regression: step size = 10\n",
    "\n",
    "- Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.\n",
    "\n",
    "- Report in tabular form the [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) value for the Training, Validation, and Testing datasets.\n",
    "- Report in tabular form  the logLossTest for the Training, Validation, and Testing datasets.\n",
    "\n",
    "- Don't forget to put a caption on your tables (above each table).\n",
    "\n",
    "###Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CriteoHelper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CriteoHelper.py\n",
    "\n",
    "from collections import defaultdict\n",
    "from pyspark.mllib.linalg import SparseVector \n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from math import log, exp\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "# hash function\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "# feature hash\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"    \n",
    "    elem = point.strip().split(',')    \n",
    "    rawFea = [(i, elem[i+1]) for i in range(len(elem) - 1)]\n",
    "    index = np.sort(hashFunction(numBuckets, rawFea, False).keys())    \n",
    "    return LabeledPoint(elem[0], SparseVector(numBuckets, index, [1]*len(index)))   \n",
    "\n",
    "# Logistic Regression Modeling & Evaluation\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w) + intercept\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1/(1+exp(-rawPrediction))\n",
    "\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12    \n",
    "    return -log(p+epsilon) if y==1 else -log(1-p+epsilon)\n",
    "\n",
    "def evaluateResults(lrModel, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"    \n",
    "    return data.map(lambda p: computeLogLoss(getP(p.features, lrModel.weights, lrModel.intercept), p.label)).mean()\n",
    "\n",
    "\n",
    "# calculate AUC\n",
    "def getAUC(rddData, lrModel):\n",
    "    labelsAndScores = rddData.map(lambda lp: (lp.label, getP(lp.features, lrModel.weights, lrModel.intercept)))\n",
    "    labelsAndWeights = labelsAndScores.collect()\n",
    "    labelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\n",
    "    labelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n",
    "\n",
    "    length = labelsByWeight.size\n",
    "    truePositives = labelsByWeight.cumsum()\n",
    "    numPositive = truePositives[-1]\n",
    "    falsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n",
    "\n",
    "    truePositiveRate = truePositives / numPositive\n",
    "    falsePositiveRate = falsePositives / (length - numPositive)\n",
    "        \n",
    "    return metrics.auc(falsePositiveRate, truePositiveRate)\n",
    "\n",
    "def logTime():\n",
    "    return str(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Criteo_Dirver_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Criteo_Dirver_1.py\n",
    "\n",
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "\n",
    "execfile('CriteoHelper.py')\n",
    "\n",
    "# define parameters\n",
    "print '%s: start logistic regression job ...' %(logTime())\n",
    "numBucketsCTR = 1000\n",
    "lrStep = 10\n",
    "start = time()\n",
    "sc = SparkContext()\n",
    "\n",
    "# data preparaion\n",
    "print '%s: preparing data ...' %(logTime())\n",
    "rawTrainData = sc.textFile('hdfs:///user/leiyang/criteo/rawTrain', 2).map(lambda x: x.replace('\\t', ','))\n",
    "rawValidationData = sc.textFile('hdfs:///user/leiyang/criteo/rawValidation', 2).map(lambda x: x.replace('\\t', ','))\n",
    "rawTestData = sc.textFile('hdfs:///user/leiyang/criteo/rawTest', 2).map(lambda x: x.replace('\\t', ','))\n",
    "\n",
    "# data encoding\n",
    "hashTrainData = rawTrainData.map(lambda p: parseHashPoint(p, numBucketsCTR))\n",
    "hashTrainData.cache()\n",
    "hashValidationData = rawValidationData.map(lambda p: parseHashPoint(p, numBucketsCTR))\n",
    "hashValidationData.cache()\n",
    "hashTestData = rawTestData.map(lambda p: parseHashPoint(p, numBucketsCTR))\n",
    "hashTestData.cache()\n",
    "\n",
    "# build model\n",
    "print '%s: building logistic regression model ...' %(logTime())\n",
    "model = LogisticRegressionWithSGD.train(hashTrainData, iterations=500, step=lrStep, regType=None, intercept=True)\n",
    "\n",
    "# get log loss\n",
    "print '%s: evaluating log loss ...' %(logTime())\n",
    "logLossVa = evaluateResults(model, hashValidationData)\n",
    "logLossTest = evaluateResults(model, hashTestData)\n",
    "logLossTrain = evaluateResults(model, hashTrainData)\n",
    "\n",
    "# get AUC\n",
    "print '%s: evaluating AUC ...' %(logTime())\n",
    "aucVal = getAUC(hashValidationData, model)\n",
    "aucTrain = getAUC(hashTrainData, model)\n",
    "aucTest = getAUC(hashTestData, model)\n",
    "print '\\n%s: job completes in %.2f minutes!' %(logTime(), (time()-start)/60.0)\n",
    "\n",
    "# show results\n",
    "print '\\n\\t\\t log loss \\t\\t\\t AUC'\n",
    "print 'Training:\\t %.4f\\t\\t %.4f' %(logLossTrain, aucTrain)\n",
    "print 'Validation:\\t %.4f\\t\\t %.4f' %(logLossVa, aucVal)\n",
    "print 'Test:\\t %.4f\\t %.4f' %(logLossTest, aucTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Submit the job to Spark\n",
    "- cluster: 1 m1.large master node + 9 m1.large task nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/usr/bin/spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode client \\\n",
    "--name LeiCriteoJob \\\n",
    "--py-files CriteoHelper.py \\\n",
    "--executor-memory '11000m' \\\n",
    "--executor-cores 2 \\\n",
    "--driver-memory '11000m' \\\n",
    "--num-executors 11 \\\n",
    "Criteo_Dirver_1.py > Criteo_log1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 13.5: Criteo Phase 2 hyperparameter tuning \n",
    "SPECIAL NOTE:\n",
    "Please share your findings as they become available with class via the Google Group. You will get brownie points for this.  Once results are shared please used them and build on them.\n",
    "\n",
    "NOTE:  please do  HW 13.5 in groups of 3 \n",
    "\n",
    "Using the training dataset, validation dataset and testing dataset in the Criteo bucket perform the following experiments:\n",
    "\n",
    "- write spark code (borrow from Phase 1 of this project) to train a logistic regression model with various hyperparamters. Do a gridsearch of the hyperparameter space and determine optimal settings using the validation set.\n",
    " - Number of buckets for hashing: 1,000, 10,000, .... explore different values  here\n",
    " - Logistic Regression: regularization term: [1e-6, 1e-3]  explore other  values here also\n",
    " - Logistic Regression: step size: explore different step sizes. Focus on a stepsize of 1 initially. \n",
    "\n",
    "- Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.\n",
    "- Report in tabular form and using heatmaps the AUC values (https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for the Training, Validation, and Testing datasets.\n",
    "- Report in tabular form and using heatmaps  the logLossTest for the Training, Validation, and Testing datasets.\n",
    "- Don't forget to put a caption on your tables (above the table) and on your heatmap figures (put caption below figures) detailing the experiment associated with each table or figure (data, algorithm used, parameters and settings explored.\n",
    "\n",
    "- Discuss the optimal setting to solve this problem  in terms of the following:\n",
    " - Features\n",
    " - Learning algortihm\n",
    " - Spark cluster\n",
    "\n",
    "Justiy your recommendations based on your experimental results and cross reference with table numbers and figure numbers. Also highlight key results with annotations, both textual and line and box based, on your tables and graphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW13.6 Heritage Healthcare Prize (OPTIONAL)\n",
    "\n",
    "The slides for Week 13 Live session contain background information for the HHH competition to predict the number of days a patient will spend in hospital.  Please review the sldies. All the data, RCode, documentation, and slides for HHH problem are located at: \n",
    "\n",
    "https://www.dropbox.com/sh/upt0j2q44ncrn1m/AAApdpXNYaEFy8KbMoE90-KSa?dl=0 \n",
    "\n",
    "In particular have a look at the following R Code:\n",
    "\n",
    "https://www.dropbox.com/s/jltk9z7jkc1o856/mainDriver.R?dl=0\n",
    "\n",
    "This code runs and will produce a baseline submission file for HHH. \n",
    "\n",
    "Challenge: \n",
    "\n",
    "Rewrite this code in Spark (all steps) and produce a submission file. Report your experimental setup and experimental times.\n",
    "\n",
    "Improve the predictive quality of your system through activities such as:\n",
    "\n",
    "-- new features\n",
    "-- feature transformations\n",
    "-- data sampling/deletion\n",
    "-- third party data\n",
    "-- learning algorithms\n",
    "-- hyperparameter tuning\n",
    "-- etc.\n",
    "\n",
    "\n",
    "State your assumptions (Training data, validation data, held out test data). Report your experimental setup and experimental times, and evaluation metrics versus the baseline submission code provided above and discuss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Start HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "resourcemanager running as process 17585. Stop it first.\n",
      "localhost: nodemanager running as process 17686. Stop it first.\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-namenode-Leis-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-datanode-Leis-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-secondarynamenode-Leis-MacBook-Pro.local.out\n",
      "historyserver running as process 18148. Stop it first.\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ start historyserver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Stop HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "no historyserver to stop\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ stop historyserver \n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
