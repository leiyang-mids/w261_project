{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "####Assignement:  week \\#9\n",
    "####[Lei Yang](mailto:leiyang@berkeley.edu) | [Michael Kennedy](mailto:mkennedy@ischool.berkeley.edu) | [Natarajan Krishnaswami](mailto:natarajan@krishnaswami.org)\n",
    "####Due: 2016-03-19, 12PM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###HW 9.0: Short answer questions\n",
    "\n",
    "What is PageRank and what is it used for in the context of web search?\n",
    "What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to \n",
    "compute the steady stade distibuton?\n",
    "OPTIONAL: In topic-specific pagerank, how can we insure that the irreducible property is satified? (HINT: see HW9.4)\n",
    "\n",
    "\n",
    "###HW 9.1: MRJob implementation of basic PageRank\n",
    "\n",
    "- Write a basic MRJob implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input (as explored in HW 7).\n",
    "- Make sure that you implementation utilizes teleportation (1-damping/the number of nodes in the network), \n",
    "and further, **distributes the mass of dangling nodes with each iteration**\n",
    "so that the output of each iteration is correctly normalized (sums to 1).\n",
    "- [**NOTE**: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,\n",
    "chooses the next page to which it will move by clicking at random, with probability d,\n",
    "one of the hyperlinks in the current page. This probability is represented by a so-called\n",
    "‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer\n",
    "jumps to any web page in the network. If a page is a dangling end, meaning it has no\n",
    "outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform\n",
    "distribution and “teleports” to that page]\n",
    "\n",
    "\n",
    "As you build your code, use the test data\n",
    "\n",
    "s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck\n",
    "your work with the true result, displayed in the first image\n",
    "in the Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "\n",
    "A,0.033\n",
    "B,0.384\n",
    "C,0.343\n",
    "D,0.039\n",
    "E,0.081\n",
    "F,0.039\n",
    "G,0.016\n",
    "H,0.016\n",
    "I,0.016\n",
    "J,0.016\n",
    "K,0.016\n",
    "\n",
    "###PageRank Iteration Job\n",
    "- **mapper**: use counter to get node count; get node struct\n",
    "- **reducer_job_init**: the loss mass count also equals to the number of dangling nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankIter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankIter.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class PageRankIter(MRJob):\n",
    "    DEFAULT_PROTOCOL = 'json'\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(PageRankIter, self).configure_options()        \n",
    "        self.add_passthrough_option(\n",
    "            '--i', dest='init', default='0', type='int',\n",
    "            help='i: run initialization iteration (default 0)')    \n",
    "\n",
    "    def mapper_job_init(self, _, line):        \n",
    "        # parse line\n",
    "        nid, adj = line.strip().split('\\t', 1)\n",
    "        nid = nid.strip('\"')\n",
    "        cmd = 'adj = %s' %adj\n",
    "        exec cmd\n",
    "        # initialize node struct        \n",
    "        node = {'a':adj.keys(), 'p':0}\n",
    "        rankMass = 1.0/len(adj)\n",
    "        # emit node\n",
    "        yield nid, node\n",
    "        # emit pageRank mass        \n",
    "        for m in node['a']:\n",
    "            yield m, rankMass\n",
    "            \n",
    "    def mapper_job_iter(self, _, line):             \n",
    "        # parse line\n",
    "        nid, node = line.strip().split('\\t', 1)\n",
    "        nid = nid.strip('\"')\n",
    "        cmd = 'node = %s' %node\n",
    "        exec cmd\n",
    "        # distribute rank mass  \n",
    "        n_adj = len(node['a'])\n",
    "        if n_adj > 0:\n",
    "            rankMass = 1.0*node['p'] / n_adj\n",
    "            # emit pageRank mass        \n",
    "            for m in node['a']:\n",
    "                yield m, rankMass\n",
    "        else:\n",
    "            # track dangling mass with counter\n",
    "            self.increment_counter('wiki_dangling_mass', 'mass', int(node['p']*1e10))\n",
    "        # reset pageRank and emit node\n",
    "        node['p'] = 0\n",
    "        yield nid, node\n",
    "    \n",
    "    def debug(self):\n",
    "        de = 'bug'\n",
    "                \n",
    "    # write a separate combiner ensure the integrity of the graph topology\n",
    "    # no additional node object will be generated\n",
    "    def combiner(self, nid, value):             \n",
    "        rankMass, node = 0.0, None        \n",
    "        # loop through all arrivals\n",
    "        for v in value:            \n",
    "            if isinstance(v, float):\n",
    "                rankMass += v                \n",
    "            else:\n",
    "                node = v            \n",
    "        # emit accumulative mass for nid       \n",
    "        if node:\n",
    "            node['p'] += rankMass\n",
    "            yield nid, node\n",
    "        else:\n",
    "            yield nid, rankMass\n",
    "    \n",
    "    # reducer for initialization pass --> need to handle dangling nodes\n",
    "    def reducer_job_init(self, nid, value):      \n",
    "        # increase counter for node count\n",
    "        self.increment_counter('wiki_node_count', 'nodes', 1)\n",
    "        rankMass, node = 0.0, None\n",
    "        # loop through all arrivals\n",
    "        for v in value:            \n",
    "            if isinstance(v, float):\n",
    "                rankMass += v         \n",
    "            else:\n",
    "                node = v\n",
    "        # handle dangling node, create node struct and add missing mass\n",
    "        if not node:            \n",
    "            node = {'a':[], 'p':rankMass}            \n",
    "            self.increment_counter('wiki_dangling_mass', 'mass', int(1e10))\n",
    "        else:\n",
    "            node['p'] += rankMass            \n",
    "        # emit for next iteration\n",
    "        yield nid, node\n",
    "        \n",
    "    # reducer for regular pass --> all nodes has structure available\n",
    "    def reducer_job_iter(self, nid, value):              \n",
    "        rankMass, node = 0.0, None\n",
    "        # loop through all arrivals\n",
    "        for v in value:            \n",
    "            if isinstance(v, float):\n",
    "                rankMass += v         \n",
    "            else:\n",
    "                node = v\n",
    "        # update pageRank\n",
    "        node['p'] += rankMass            \n",
    "        # emit for next iteration\n",
    "        yield nid, node\n",
    "\n",
    "    def steps(self):\n",
    "        jc = {\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '2',\n",
    "        }\n",
    "        return [MRStep(mapper=self.mapper_job_init if self.options.init else self.mapper_job_iter                       \n",
    "                       , combiner=self.combiner                       \n",
    "                       , reducer=self.reducer_job_init if self.options.init else self.reducer_job_iter\n",
    "                       , jobconf = jc\n",
    "                      )\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PageRankIter.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### unit test #####\n",
    "#!python PageRankIter.py ./data/PageRank-test.txt --i 1 -r 'inline'  > iter1.t\n",
    "#!python PageRankIter.py iter8.t --i 0 -r 'inline' > iter9.t\n",
    "#!python PageRankIter.py 'hdfs:///user/leiyang/PageRank-test.txt' --i 1 -r 'hadoop' --output-dir 's3://w261.data/HW9_test' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PageRankDist Job\n",
    "- applying damping and random jump factor\n",
    "- redistribute dangling mass across the graph\n",
    "- note: normalize the ranking number at last iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankDist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankDist.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class PageRankDist(MRJob):\n",
    "    DEFAULT_PROTOCOL = 'json'\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(PageRankDist, self).configure_options()        \n",
    "        self.add_passthrough_option(\n",
    "            '--s', dest='size', default=0, type='int',\n",
    "            help='size: node number (default 0)')    \n",
    "        self.add_passthrough_option(\n",
    "            '--j', dest='alpha', default=0.15, type='float',\n",
    "            help='jump: random jump factor (default 0.15)') \n",
    "        self.add_passthrough_option(\n",
    "            '--n', dest='norm', default=0, type='int',\n",
    "            help='norm: normalize pageRank with graph size (default 0)') \n",
    "        self.add_passthrough_option(\n",
    "            '--m', dest='m', default=0, type='float',\n",
    "            help='m: rank mass from dangling nodes (default 0)') \n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.damping = 1 - self.options.alpha        \n",
    "        self.p_dangling = self.options.m / self.options.size        \n",
    "    \n",
    "    # needed after initialization, after node number becomes available\n",
    "    def mapper_norm(self, _, line):        \n",
    "        # parse line\n",
    "        nid, node = line.strip().split('\\t', 1)\n",
    "        nid = nid.strip('\"')\n",
    "        cmd = 'node = %s' %node\n",
    "        exec cmd\n",
    "        # get final pageRank      \n",
    "        node['p'] = ((self.p_dangling + node['p'])*self.damping+self.options.alpha) / self.options.size\n",
    "        yield nid, node\n",
    "            \n",
    "    def mapper(self, _, line):             \n",
    "        # parse line\n",
    "        nid, node = line.strip().split('\\t', 1)\n",
    "        nid = nid.strip('\"')\n",
    "        cmd = 'node = %s' %node\n",
    "        exec cmd\n",
    "        # get final pageRank      \n",
    "        node['p'] = (self.p_dangling + node['p']) * self.damping + self.options.alpha\n",
    "        yield nid, node\n",
    "\n",
    "    def steps(self):\n",
    "        jc = {\n",
    "            'mapreduce.job.maps': '2',           \n",
    "        }\n",
    "        return [MRStep(mapper_init=self.mapper_init\n",
    "                       , mapper=self.mapper_norm if self.options.norm else self.mapper                       \n",
    "                       , jobconf = jc\n",
    "                      )\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PageRankDist.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### unit test #####\n",
    "\n",
    "#!python PageRankIter.py ./data/PageRank-test.txt --i 1 -r 'inline'  > iter1.t\n",
    "#!python PageRankIter.py iter8.t --i 0 -r 'inline' > iter9.t\n",
    "\n",
    "\n",
    "#!python PageRankDist.py iter1.t --n 0 --s 11 --j 0.15 --m 1.0 > iter2.t\n",
    "#!python PageRankDist.py iter9.t --n 0 --s 11 --j 0.15 --m 0.41355561274607455   > iter10.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PageRankSort Job\n",
    "- sort pageRank with descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankSort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankSort.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class PageRankSort(MRJob):\n",
    "    #DEFAULT_PROTOCOL = 'json'   \n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(PageRankSort, self).configure_options()        \n",
    "        self.add_passthrough_option(\n",
    "            '--s', dest='size', default=0, type='int',\n",
    "            help='size: node number (default 0)')\n",
    "        self.add_passthrough_option(\n",
    "            '--n', dest='top', default=100, type='int',\n",
    "            help='size: node number (default 100)')\n",
    "    \n",
    "    def mapper(self, _, line):        \n",
    "        # parse line\n",
    "        nid, node = line.strip().split('\\t', 1)\n",
    "        cmd = 'node = %s' %node\n",
    "        exec cmd        \n",
    "        yield node['p'], nid.strip('\"')\n",
    "    \n",
    "    def reducer_init(self):        \n",
    "        self.i = 0\n",
    "        self.total = 0\n",
    "    \n",
    "    def reducer(self, pageRank, nid): \n",
    "        for n in nid:\n",
    "            if self.i < self.options.top:\n",
    "                self.i += 1\n",
    "                self.total += pageRank\n",
    "                yield n, pageRank/self.options.size\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        yield 'total mass: ', self.total/self.options.size\n",
    "\n",
    "    def steps(self):\n",
    "        jc = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "            'mapreduce.job.maps': '2', \n",
    "            'mapreduce.job.reduces': '1',  # must be 1 for sorting\n",
    "        }\n",
    "        return [MRStep(mapper=self.mapper, reducer_init=self.reducer_init, \n",
    "                       reducer=self.reducer, reducer_final=self.reducer_final\n",
    "                       , jobconf = jc\n",
    "                      )\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PageRankSort.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### unit test #####\n",
    "#!python PageRankSort.py iter10.t --s 11 --n 100 -r 'hadoop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PageRankJoin Job\n",
    "- find page name from index file for the top ranked pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankJoin.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "class PageRankJoin(MRJob):\n",
    "    #DEFAULT_PROTOCOL = 'json'   \n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.topRanks = {}\n",
    "        # read rand list, prepare for mapper in-memory join        \n",
    "        cat = Popen(['cat', 'part-00000'], stdout=PIPE)\n",
    "        for line in cat.stdout:\n",
    "            nid, rank = line.strip().split('\\t')\n",
    "            self.topRanks[nid.strip('\"')] = rank\n",
    "    \n",
    "    def mapper(self, _, line):        \n",
    "        # parse line\n",
    "        name, nid, d_in, d_out = line.strip().split('\\t')\n",
    "        if nid in self.topRanks:                        \n",
    "            yield float(self.topRanks[nid]), '%s - %s' %(nid, name)\n",
    "    \n",
    "    def reducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield key, v\n",
    "    \n",
    "\n",
    "    def steps(self):\n",
    "        jc = {            \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "            'mapreduce.job.maps': '2', \n",
    "            'mapreduce.job.reduces': '1',  # must be 1 for sorting            \n",
    "        }\n",
    "        return [MRStep(mapper_init=self.mapper_init\n",
    "                       , mapper=self.mapper\n",
    "                       , reducer=self.reducer\n",
    "                       , jobconf = jc\n",
    "                      )\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PageRankJoin.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### unit test #####\n",
    "#!python PageRankJoin.py 'PageRankIndex' -r 'hadoop' --file 'rank' #> test.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import requests\n",
    "\n",
    "def getCounter(groupName, counterName, host = 'localhost'):\n",
    "    # get job list       \n",
    "    getJobs = 'http://%s:19888/ws/v1/history/mapreduce/jobs' %host\n",
    "    jobs = requests.get(getJobs).json()['jobs']['job'] \n",
    "    # get counters\n",
    "    ts = max([job['finishTime'] for job in jobs])\n",
    "    id = [job['id'] for job in jobs if job['finishTime'] == ts][0]\n",
    "    getCounters = 'http://%s:19888/ws/v1/history/mapreduce/jobs/%s/counters' %(host, id)\n",
    "    counterGroups = requests.get(getCounters).json()['jobCounters']['counterGroup']\n",
    "    # loop through to counters to return value\n",
    "    counters = [g['counter'] for g in counterGroups if g['counterGroupName']==groupName][0]\n",
    "    totalValues = [c['totalCounterValue'] for c in counters if c['name']==counterName]\n",
    "    return totalValues[0] if len(totalValues)==1 else None\n",
    "\n",
    "def getCounters(groupName, host = 'localhost'):\n",
    "    # get job list       \n",
    "    getJobs = 'http://%s:19888/ws/v1/history/mapreduce/jobs' %host\n",
    "    jobs = requests.get(getJobs).json()['jobs']['job'] \n",
    "    # get counters\n",
    "    ts = max([job['finishTime'] for job in jobs])\n",
    "    id = [job['id'] for job in jobs if job['finishTime'] == ts][0]\n",
    "    getCounters = 'http://%s:19888/ws/v1/history/mapreduce/jobs/%s/counters' %(host, id)\n",
    "    counterGroups = requests.get(getCounters).json()['jobCounters']['counterGroup']\n",
    "    # loop through to counters to return value\n",
    "    counters = [g['counter'] for g in counterGroups if g['counterGroupName']==groupName]    \n",
    "    return {c['name']:c['totalCounterValue'] for c in counters[0]} if len(counters)==1 else []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'mass_0': 10000000000,\n",
       " u'mass_1': 10000000000,\n",
       " u'mass_10': 10000000000,\n",
       " u'mass_2': 10000000000,\n",
       " u'mass_3': 10000000000,\n",
       " u'mass_4': 10000000000,\n",
       " u'mass_5': 10000000000,\n",
       " u'mass_6': 10000000000,\n",
       " u'mass_7': 10000000000,\n",
       " u'mass_8': 10000000000,\n",
       " u'mass_9': 10000000000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### unit test #####\n",
    "#http://localhost:19888/ws/v1/history/mapreduce/jobs/job_1457742616221_0001/counters\n",
    "#from helper import getCounter, getCounters\n",
    "#getCounter('wiki_node_count', 'nodes', 'ec2-52-87-184-124.compute-1.amazonaws.com')\n",
    "#getCounters('wiki_dangling_mass')\n",
    "#getCounter('org.apache.hadoop.mapreduce.JobCounter', 'TOTAL_LAUNCHED_MAPS')\n",
    "#getCounters('org.apache.hadoop.mapreduce.JobCounter', 'ec2-54-172-84-241.compute-1.amazonaws.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PageRank Driver\n",
    "- initialize the process: \n",
    " - get node count, and dangling node count\n",
    " - redistribute loss mass, apply jump/damping factor\n",
    "- get loss mass from counter\n",
    "- iterately execute pageRank:\n",
    " - run pageRank process\n",
    " - get loss mass\n",
    "- sort rank, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting RunPageRank.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile RunPageRank.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from PageRankIter import PageRankIter\n",
    "from PageRankDist import PageRankDist\n",
    "from PageRankSort import PageRankSort\n",
    "from PageRankJoin import PageRankJoin\n",
    "from helper import getCounter\n",
    "from subprocess import call, check_output\n",
    "from time import time\n",
    "import sys, getopt, datetime, os\n",
    "\n",
    "# parse parameter\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(sys.argv[1:], \"hg:j:i:d:s:\")\n",
    "    except getopt.GetoptError:\n",
    "        print 'RunBFS.py -g <graph> -j <jump> -i <iteration> -d <index> -s <size>'\n",
    "        sys.exit(2)\n",
    "    if len(opts) != 5:\n",
    "        print 'RunBFS.py -g <graph> -j <jump> -i <iteration> -d <index>'\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print 'RunBFS.py -g <graph> -j <jump> -i <iteration> -d <index>'\n",
    "            sys.exit(2)\n",
    "        elif opt == '-g':\n",
    "            graph = arg\n",
    "        elif opt == '-j':\n",
    "            jump = arg\n",
    "        elif opt == '-i':            \n",
    "            n_iter = arg\n",
    "        elif opt == '-d':\n",
    "            index = arg\n",
    "        elif opt == '-s':\n",
    "            n_node = arg\n",
    "        \n",
    "start = time()\n",
    "FNULL = open(os.devnull, 'w')\n",
    "n_iter = int(n_iter)\n",
    "doJoin = index!='NULL'\n",
    "doInit = n_node=='0'\n",
    "host = 'localhost'\n",
    "\n",
    "print '%s: %s PageRanking on \\'%s\\' for %d iterations with damping factor %.2f ...' %(str(datetime.datetime.now()),\n",
    "          'start' if doInit else 'continue', graph[graph.rfind('/')+1:], n_iter, 1-float(jump))\n",
    "\n",
    "if doInit:\n",
    "    # clear directory\n",
    "    print str(datetime.datetime.now()) + ': clearing directory ...'\n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/in'], stdout=FNULL)\n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/out'], stdout=FNULL)\n",
    "    \n",
    "    # creat initialization job    \n",
    "    init_job = PageRankIter(args=[graph, '--i', '1', '-r', 'hadoop', '--output-dir', 'hdfs:///user/leiyang/out'])\n",
    "\n",
    "    # run initialization job\n",
    "    print str(datetime.datetime.now()) + ': running iteration 1 ...'\n",
    "    with init_job.make_runner() as runner:    \n",
    "        runner.run()\n",
    "\n",
    "    # checking counters\n",
    "    n_node = getCounter('wiki_node_count', 'nodes', host)\n",
    "    n_dangling = getCounter('wiki_dangling_mass', 'mass', host)/1e10\n",
    "    print '%s: initialization complete: %d nodes, %d are dangling!' %(str(datetime.datetime.now()), n_node, n_dangling)\n",
    "\n",
    "    # run redistribution job\n",
    "    call(['hdfs', 'dfs', '-mv', '/user/leiyang/out', '/user/leiyang/in'])\n",
    "    dist_job = PageRankDist(args=['hdfs:///user/leiyang/in/part*', '--s', str(n_node), '--j', jump, '--n', '0', \n",
    "                                '--m', str(n_dangling), '-r', 'hadoop', '--output-dir', 'hdfs:///user/leiyang/out'])\n",
    "    print str(datetime.datetime.now()) + ': distributing loss mass ...'\n",
    "    with dist_job.make_runner() as runner:    \n",
    "        runner.run()\n",
    "\n",
    "# move results for next iteration\n",
    "call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/in'], stdout=FNULL)\n",
    "call(['hdfs', 'dfs', '-mv', '/user/leiyang/out', '/user/leiyang/in'])\n",
    "\n",
    "# create iteration job\n",
    "iter_job = PageRankIter(args=['hdfs:///user/leiyang/in/part*', '--i', '0', \n",
    "                              '-r', 'hadoop', '--output-dir', 'hdfs:///user/leiyang/out'])\n",
    "\n",
    "# run pageRank iteratively\n",
    "i = 2 if doInit else 1\n",
    "while(1):\n",
    "    print str(datetime.datetime.now()) + ': running iteration %d ...' %i\n",
    "    with iter_job.make_runner() as runner:        \n",
    "        runner.run()\n",
    "    \n",
    "    # check counter for loss mass\n",
    "    mass_loss = getCounter('wiki_dangling_mass', 'mass', host)/1e10\n",
    "    \n",
    "    # move results for next iteration\n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/in'], stdout=FNULL)\n",
    "    call(['hdfs', 'dfs', '-mv', '/user/leiyang/out', '/user/leiyang/in'])\n",
    "        \n",
    "    # run redistribution job\n",
    "    dist_job = PageRankDist(args=['hdfs:///user/leiyang/in/part*', '--s', str(n_node), '--j', jump, '--n', '0', \n",
    "                                '--m', str(mass_loss), '-r', 'hadoop', '--output-dir', 'hdfs:///user/leiyang/out'])\n",
    "    print str(datetime.datetime.now()) + ': distributing loss mass %.4f ...' %mass_loss\n",
    "    with dist_job.make_runner() as runner:    \n",
    "        runner.run()\n",
    "    \n",
    "    if i == n_iter:\n",
    "        break\n",
    "    \n",
    "    # if more iteration needed\n",
    "    i += 1    \n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/in'], stdout=FNULL)\n",
    "    call(['hdfs', 'dfs', '-mv', '/user/leiyang/out', '/user/leiyang/in'], stdout=FNULL)\n",
    "\n",
    "# run sort job\n",
    "print str(datetime.datetime.now()) + ': sorting PageRank ...'\n",
    "call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/rank'], stdout=FNULL)\n",
    "sort_job = PageRankSort(args=['hdfs:///user/leiyang/out/part*', '--s', str(n_node), '--n', '100',\n",
    "                              '-r', 'hadoop', '--output-dir', 'hdfs:///user/leiyang/rank'])\n",
    "with sort_job.make_runner() as runner:    \n",
    "    runner.run()\n",
    "    \n",
    "# run join job\n",
    "if doJoin:\n",
    "    print str(datetime.datetime.now()) + ': joining PageRank with index ...'\n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/join'], stdout=FNULL)\n",
    "    join_job = PageRankJoin(args=[index, '-r', 'hadoop', '--file', 'hdfs:///user/leiyang/rank/part-00000', \n",
    "                                  '--output-dir', 'hdfs:///user/leiyang/join'])\n",
    "    with join_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "print \"%s: PageRank job completes in %.1f minutes!\\n\" %(str(datetime.datetime.now()), (time()-start)/60.0)\n",
    "\n",
    "# TODO\n",
    "# copy results to S3: \n",
    "#call(['hdfs', 'dfs', '-cat', '/user/leiyang/join/p*' if doJoin else '/user/leiyang/rank/p*', '>', 'results_wiki_50'])\n",
    "#call(['aws', 's3', 'cp', 'results_wiki_50', 's3://w261.data/HW9/results_wiki_50', '--region', 'us-west-2'])\n",
    "# terminate cluster: - aws emr terminate-clusters --cluster-ids j-CT2NH23KIIBL\n",
    "#print \"%s: results copied to S3, shutting down cluster ...\" %(str(datetime.datetime.now()))\n",
    "#call(['aws', 'emr', 'terminate-clusters', '--cluster-ids', 'j-CT2NH23KIIBL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-12 19:29:27.005871: start PageRanking on 'PageRank-test.txt' for 2 iterations with damping factor 0.85 ...\n",
      "2016-03-12 19:29:27.005931: clearing directory ...\n",
      "rm: `/user/leiyang/in': No such file or directory\n",
      "rm: `/user/leiyang/out': No such file or directory\n",
      "2016-03-12 19:29:30.189038: running iteration 1 ...\n",
      "No handlers could be found for logger \"mrjob.conf\"\n",
      "2016-03-12 19:30:02.277672: initialization complete: 11 nodes, 1 are dangling!\n",
      "2016-03-12 19:30:03.906794: distributing loss mass ...\n",
      "2016-03-12 19:30:30.962587: running iteration 2 ...\n",
      "2016-03-12 19:31:03.836293: distributing loss mass 0.6523 ...\n",
      "2016-03-12 19:31:28.939551: sorting PageRank ...\n",
      "rm: `/user/leiyang/rank': No such file or directory\n",
      "2016-03-12 19:32:00.800891: joining PageRank with index ...\n",
      "rm: `/user/leiyang/join': No such file or directory\n",
      "2016-03-12 19:32:30.232733: PageRank job completes in 3.1 minutes!\n",
      "\n",
      "0.2875607312792286\t\"C - !T.O.O.H.!\"\n",
      "0.2606908965684848\t\"B - !Kung language\"\n",
      "0.11164819684396692\t\"D - !Kung San\"\n",
      "0.09941334835911846\t\"E - !Que Buena Se Puso Lola!\"\n",
      "0.03794640621035812\t\"A - !Oka Tokat\"\n",
      "0.01821844477785124\t\"I - !Kweiten ta //ken\"\n",
      "0.01821844477785124\t\"H - !Kung\"\n",
      "0.01821844477785124\t\"G - !Kung people\"\n"
     ]
    }
   ],
   "source": [
    "##### unit test #####\n",
    "!python RunPageRank.py -g 'hdfs:///user/leiyang/PageRank-test.txt' -j 0.15 -i 2 \\\n",
    "-d 'hdfs:///user/leiyang/PageRankIndex' -s '0'\n",
    "\n",
    "#!python RunPageRank.py -g 'hdfs:///user/leiyang/PageRank-test.txt' -j 0.15 -i 2 \\\n",
    "#-d 'hdfs:///user/leiyang/PageRankIndex' -s '11'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "###HW 9.2: Exploring PageRank teleportation and network plots\n",
    "In order to overcome  problems such as disconnected components, the damping factor (a typical value for d is 0.85) can be varied. \n",
    "Using the graph in HW 9.1, plot the test graph (using networkx, https://networkx.github.io/) for several values of the damping parameter alpha,\n",
    "so that each nodes radius is proportional to its PageRank score. In particular you should\n",
    "do this for the following damping factors: $[0,0.25,0.5,0.75, 0.85, 1]$. Note your plots should look like the following:\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!python RunPageRank.py -g 'hdfs:///user/leiyang/PageRank-test.txt' -j 1 -i 3 -d 'NULL'  -c 'n'\n",
    "#!hdfs dfs -cat /user/leiyang/rank/p* > HW_9_2_0\n",
    "#!python RunPageRank.py -g 'hdfs:///user/leiyang/PageRank-test.txt' -j 0.75 -i 10 -d 'NULL' -c 'n'\n",
    "#!hdfs dfs -cat /user/leiyang/rank/p* > HW_9_2_1\n",
    "#!python RunPageRank.py -g 'hdfs:///user/leiyang/PageRank-test.txt' -j 0.5 -i 10 -d 'NULL' -c 'n'\n",
    "#!hdfs dfs -cat /user/leiyang/rank/p* > HW_9_2_2\n",
    "#!python RunPageRank.py -g 'hdfs:///user/leiyang/PageRank-test.txt' -j 0.25 -i 10 -d 'NULL' -c 'n'\n",
    "#!hdfs dfs -cat /user/leiyang/rank/p* > HW_9_2_3\n",
    "#!python RunPageRank.py -g 'hdfs:///user/leiyang/PageRank-test.txt' -j 0.15 -i 10 -d 'NULL' -c 'n'\n",
    "#!hdfs dfs -cat /user/leiyang/rank/p* > HW_9_2_4\n",
    "#!python RunPageRank.py -g 'hdfs:///user/leiyang/PageRank-test.txt' -j 0 -i 10 -d 'NULL' -c 'n'\n",
    "#!hdfs dfs -cat /user/leiyang/rank/p* > HW_9_2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFBCAYAAABEo8fdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x165ed3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "G=nx.DiGraph()\n",
    "G.add_node(\"spam\")\n",
    "G.add_node(\"ham\")\n",
    "G.add_node(\"test\")\n",
    "G.add_edge(\"spam\",\"ham\")\n",
    "G.add_edge('test','ham')\n",
    "nx.draw(G, node_size=[100,200,800])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###HW 9.3: Applying PageRank to the Wikipedia hyperlinks network\n",
    "\n",
    "- Run your PageRank implementation on the Wikipedia dataset for 10 iterations,\n",
    "and display the top 100 ranked nodes (with alpha = 0.85).\n",
    "- Run your PageRank implementation on the Wikipedia dataset for 50 iterations,\n",
    "and display the top 100 ranked nodes (with teleportation factor of 0.15). \n",
    "\n",
    "- Have the top 100 ranked pages changed? \n",
    " - Comment on your findings. \n",
    " - Plot the pagerank values for the top 100 pages resulting from the 50 iterations run. \n",
    " - Then plot the pagerank values for the same 100 pages that resulted from the 10 iterations run.  \n",
    " \n",
    "### Top 100 PageRank on 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014614491942438349\t\"13455888 - United States\"\r\n",
      "0.0006663317375798755\t\"1184351 - Animal\"\r\n",
      "0.0006398051875506791\t\"4695850 - France\"\r\n",
      "0.0005748538826286757\t\"5051368 - Germany\"\r\n",
      "0.0004503047142785041\t\"1384888 - Arthropod\"\r\n",
      "0.00044660099098876925\t\"2437837 - Canada\"\r\n",
      "0.0004448182480165896\t\"6113490 - Insect\"\r\n",
      "0.000444203734598283\t\"7902219 - List of sovereign states\"\r\n",
      "0.0004329952449265189\t\"13425865 - United Kingdom\"\r\n",
      "0.000427885336130834\t\"6076759 - India\"\r\n",
      "0.00042327595490625425\t\"4196067 - England\"\r\n",
      "0.00039817029586927596\t\"6172466 - Iran\"\r\n",
      "0.0003854336708092153\t\"14112583 - World War II\"\r\n",
      "0.0003631670660462813\t\"10390714 - Poland\"\r\n",
      "0.00034383110763123157\t\"15164193 - village\"\r\n",
      "0.0003383478652193903\t\"3191491 - Countries of the world\"\r\n",
      "0.0003293524626579213\t\"6416278 - Japan\"\r\n",
      "0.00032896996556415974\t\"6237129 - Italy\"\r\n",
      "0.00032632071986423984\t\"7835160 - List of countries\"\r\n",
      "0.0003250758881557822\t\"1516699 - Australia\"\r\n",
      "0.0003131434418030617\t\"13725487 - Voivodeships of Poland\"\r\n",
      "0.0003095941242456258\t\"9276255 - National Register of Historic Places\"\r\n",
      "0.00030809546897991505\t\"7576704 - Lepidoptera\"\r\n",
      "0.0003035425666087729\t\"10469541 - Powiat\"\r\n",
      "0.0002979533522103301\t\"5154210 - Gmina\"\r\n",
      "0.0002857902942672035\t\"12836211 - The New York Times\"\r\n",
      "0.00028347554322906003\t\"7990491 - London\"\r\n",
      "0.0002690621118260582\t\"4198751 - English language\"\r\n",
      "0.00026401327504910687\t\"2797855 - China\"\r\n",
      "0.0002610656557463978\t\"11253108 - Russia\"\r\n",
      "0.00025755868482122047\t\"9386580 - New York City\"\r\n",
      "0.0002550899370909594\t\"3603527 - Departments of France\"\r\n",
      "0.00025104301138297773\t\"12074312 - Spain\"\r\n",
      "0.0002487901833478194\t\"3069099 - Communes of France\"\r\n",
      "0.0002454573288537552\t\"14881689 - moth\"\r\n",
      "0.0002448490318480156\t\"2155467 - Brazil\"\r\n",
      "0.00023872444275286543\t\"1441065 - Association football\"\r\n",
      "0.00023335074718638498\t\"14503460 - association football\"\r\n",
      "0.00022060503331685166\t\"2396749 - California\"\r\n",
      "0.00021509725578691428\t\"3191268 - Counties of Iran\"\r\n",
      "0.00021468682890528565\t\"10566120 - Provinces of Iran\"\r\n",
      "0.000211379096562195\t\"2614581 - Central European Time\"\r\n",
      "0.00021132415993817432\t\"11147327 - Romania\"\r\n",
      "0.00020715963504228723\t\"1637982 - Bakhsh\"\r\n",
      "0.00020338117266915535\t\"12430985 - Sweden\"\r\n",
      "0.0002026232339501364\t\"11245362 - Rural Districts of Iran\"\r\n",
      "0.00019701920174091924\t\"9355455 - Netherlands\"\r\n",
      "0.00019142274072843148\t\"10527224 - Private Use Areas\"\r\n",
      "0.00019074389254287321\t\"14112408 - World War I\"\r\n",
      "0.00018818343629020115\t\"2614578 - Central European Summer Time\"\r\n",
      "0.00018809311929740536\t\"9391762 - New York\"\r\n",
      "0.0001871031700113471\t\"8697871 - Mexico\"\r\n",
      "0.00018685330248519627\t\"6172167 - Iran Standard Time\"\r\n",
      "0.00018540138485575856\t\"981395 - AllMusic\"\r\n",
      "0.00017885001537615815\t\"6171937 - Iran Daylight Time\"\r\n",
      "0.00017834740118144265\t\"5490435 - Hangul\"\r\n",
      "0.00017325786783489066\t\"11582765 - Scotland\"\r\n",
      "0.00016954981151296857\t\"14725161 - gene\"\r\n",
      "0.00016767695230721733\t\"12067030 - Soviet Union\"\r\n",
      "0.00016731685713821892\t\"9562547 - Norway\"\r\n",
      "0.00016548126934061455\t\"994890 - Allmusic\"\r\n",
      "0.00016067308336923405\t\"9997298 - Paris\"\r\n",
      "0.00016052821854839323\t\"9394907 - New Zealand\"\r\n",
      "0.0001590426968518198\t\"13280859 - Turkey\"\r\n",
      "0.00015776886151301826\t\"10345830 - Plant\"\r\n",
      "0.00015530367287750363\t\"4978429 - Geographic Names Information System\"\r\n",
      "0.00015495000505848502\t\"12447593 - Switzerland\"\r\n",
      "0.00015322797217141157\t\"8019937 - Los Angeles\"\r\n",
      "0.00014889113577784647\t\"11148415 - Romanize\"\r\n",
      "0.00014788104548259728\t\"13432150 - United States Census Bureau\"\r\n",
      "0.00014712672457525987\t\"4344962 - Europe\"\r\n",
      "0.00014192897863341285\t\"1175360 - Angiosperms\"\r\n",
      "0.0001413128675411494\t\"12038331 - South Africa\"\r\n",
      "0.0001390960411124565\t\"14565507 - census\"\r\n",
      "0.00013781320406146952\t\"4624519 - Flowering plant\"\r\n",
      "0.00013627013406909209\t\"1523975 - Austria\"\r\n",
      "0.00013494958518642523\t\"14981725 - protein\"\r\n",
      "0.00013474185796926125\t\"13328060 - U.S. state\"\r\n",
      "0.0001307386712115197\t\"1332806 - Argentina\"\r\n",
      "0.0001302313942366781\t\"10399499 - Political divisions of the United States\"\r\n",
      "0.00013006969329161375\t\"14963657 - population density\"\r\n",
      "0.00012834313002134818\t\"2578813 - Catholic Church\"\r\n",
      "0.00012828888425440012\t\"2826544 - Chordate\"\r\n",
      "0.00012723774168965516\t\"1575979 - BBC\"\r\n",
      "0.00012713810216423896\t\"1813634 - Belgium\"\r\n",
      "0.00012404627523796507\t\"2778099 - Chicago\"\r\n",
      "0.0001208387713656\t\"13853369 - Washington, D.C.\"\r\n",
      "0.00012028369796106582\t\"9924814 - Pakistan\"\r\n",
      "0.00011582953101167437\t\"4568647 - Finland\"\r\n",
      "0.0001144336932340961\t\"12785678 - The Guardian\"\r\n",
      "0.00011442817412578247\t\"7467127 - Latin\"\r\n",
      "0.00011425740363317814\t\"9742161 - Ontario\"\r\n",
      "0.00011368488062447509\t\"3328327 - Czech Republic\"\r\n",
      "0.00011328616898369108\t\"10246542 - Philippines\"\r\n",
      "0.00011326731774131182\t\"3591832 - Denmark\"\r\n",
      "0.00011319055333767748\t\"5274313 - Greece\"\r\n",
      "0.00011298651394752375\t\"14727077 - genus\"\r\n",
      "0.00011246839612227319\t\"14709489 - football (soccer)\"\r\n",
      "0.00011223456345201143\t\"5908108 - Hungary\"\r\n",
      "0.00011220084056744828\t\"3973000 - Eastern European Time\"\r\n"
     ]
    }
   ],
   "source": [
    "#!python RunPageRank.py -g 'hdfs:///user/leiyang/all-pages-indexed-out.txt' -j 0.15 -i 10 \\\n",
    "#-d 'hdfs:///user/leiyang/indices.txt' -s '0'\n",
    "!cat ./data/wiki_10_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 PageRank on 50 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014615599816380814\t\"13455888 - United States\"\r\n",
      "0.0006660177936038597\t\"1184351 - Animal\"\r\n",
      "0.0006396773757180422\t\"4695850 - France\"\r\n",
      "0.0005747671982893716\t\"5051368 - Germany\"\r\n",
      "0.0004501232221973807\t\"1384888 - Arthropod\"\r\n",
      "0.00044667005168115624\t\"2437837 - Canada\"\r\n",
      "0.00044463224402460465\t\"6113490 - Insect\"\r\n",
      "0.00044387869965694206\t\"7902219 - List of sovereign states\"\r\n",
      "0.00043314218173262273\t\"13425865 - United Kingdom\"\r\n",
      "0.00042770776770628867\t\"6076759 - India\"\r\n",
      "0.00042341679596246464\t\"4196067 - England\"\r\n",
      "0.000397826042012343\t\"6172466 - Iran\"\r\n",
      "0.00038548623796188223\t\"14112583 - World War II\"\r\n",
      "0.00036266653367941786\t\"10390714 - Poland\"\r\n",
      "0.00034358745300642004\t\"15164193 - village\"\r\n",
      "0.0003380496128621886\t\"3191491 - Countries of the world\"\r\n",
      "0.0003292203268728178\t\"6416278 - Japan\"\r\n",
      "0.00032899474579557773\t\"6237129 - Italy\"\r\n",
      "0.00032620175381522067\t\"7835160 - List of countries\"\r\n",
      "0.00032511085571704744\t\"1516699 - Australia\"\r\n",
      "0.00031268227722189133\t\"13725487 - Voivodeships of Poland\"\r\n",
      "0.0003095692741012243\t\"9276255 - National Register of Historic Places\"\r\n",
      "0.00030798064678708647\t\"7576704 - Lepidoptera\"\r\n",
      "0.0003031203814549852\t\"10469541 - Powiat\"\r\n",
      "0.0002975477873102353\t\"5154210 - Gmina\"\r\n",
      "0.00028603760467156336\t\"12836211 - The New York Times\"\r\n",
      "0.0002836201779820457\t\"7990491 - London\"\r\n",
      "0.00026905355560406986\t\"4198751 - English language\"\r\n",
      "0.00026401414743470433\t\"2797855 - China\"\r\n",
      "0.0002609847438047289\t\"11253108 - Russia\"\r\n",
      "0.00025769760112647513\t\"9386580 - New York City\"\r\n",
      "0.0002549708627794919\t\"3603527 - Departments of France\"\r\n",
      "0.0002510220915988287\t\"12074312 - Spain\"\r\n",
      "0.00024867559431367516\t\"3069099 - Communes of France\"\r\n",
      "0.00024536414137746424\t\"14881689 - moth\"\r\n",
      "0.00024471986910370725\t\"2155467 - Brazil\"\r\n",
      "0.00023864828925466942\t\"1441065 - Association football\"\r\n",
      "0.00023330403431633522\t\"14503460 - association football\"\r\n",
      "0.00022063223474869253\t\"2396749 - California\"\r\n",
      "0.0002149554605041878\t\"3191268 - Counties of Iran\"\r\n",
      "0.0002145445586070783\t\"10566120 - Provinces of Iran\"\r\n",
      "0.0002112031979712803\t\"2614581 - Central European Time\"\r\n",
      "0.00021118711279724468\t\"11147327 - Romania\"\r\n",
      "0.00020703164634469528\t\"1637982 - Bakhsh\"\r\n",
      "0.00020330214007671839\t\"12430985 - Sweden\"\r\n",
      "0.00020252992610044653\t\"11245362 - Rural Districts of Iran\"\r\n",
      "0.00019701419936357027\t\"9355455 - Netherlands\"\r\n",
      "0.00019139065958623014\t\"10527224 - Private Use Areas\"\r\n",
      "0.0001907835867254496\t\"14112408 - World War I\"\r\n",
      "0.0001881715264835644\t\"9391762 - New York\"\r\n",
      "0.0001880220706466143\t\"2614578 - Central European Summer Time\"\r\n",
      "0.00018704386395116136\t\"8697871 - Mexico\"\r\n",
      "0.000186732570547333\t\"6172167 - Iran Standard Time\"\r\n",
      "0.00018522886640443312\t\"981395 - AllMusic\"\r\n",
      "0.00017874919317338242\t\"6171937 - Iran Daylight Time\"\r\n",
      "0.00017831292465815966\t\"5490435 - Hangul\"\r\n",
      "0.0001733486967173185\t\"11582765 - Scotland\"\r\n",
      "0.00016948367863767668\t\"14725161 - gene\"\r\n",
      "0.00016765208013989067\t\"12067030 - Soviet Union\"\r\n",
      "0.0001672147998973972\t\"9562547 - Norway\"\r\n",
      "0.00016539998297863457\t\"994890 - Allmusic\"\r\n",
      "0.000160696299067241\t\"9997298 - Paris\"\r\n",
      "0.00016052347832283404\t\"9394907 - New Zealand\"\r\n",
      "0.000159006634081669\t\"13280859 - Turkey\"\r\n",
      "0.00015761805655622823\t\"10345830 - Plant\"\r\n",
      "0.00015527176841381843\t\"4978429 - Geographic Names Information System\"\r\n",
      "0.00015493020349201966\t\"12447593 - Switzerland\"\r\n",
      "0.00015329019745546487\t\"8019937 - Los Angeles\"\r\n",
      "0.00014883434413415624\t\"11148415 - Romanize\"\r\n",
      "0.00014785698802403393\t\"13432150 - United States Census Bureau\"\r\n",
      "0.00014711081037696502\t\"4344962 - Europe\"\r\n",
      "0.0001418440693202807\t\"1175360 - Angiosperms\"\r\n",
      "0.00014129930023312176\t\"12038331 - South Africa\"\r\n",
      "0.00013906672709870576\t\"14565507 - census\"\r\n",
      "0.00013764660835955233\t\"4624519 - Flowering plant\"\r\n",
      "0.00013624635676277595\t\"1523975 - Austria\"\r\n",
      "0.00013489587624568975\t\"14981725 - protein\"\r\n",
      "0.00013474291497490742\t\"13328060 - U.S. state\"\r\n",
      "0.00013069327812236255\t\"1332806 - Argentina\"\r\n",
      "0.0001302063045995383\t\"10399499 - Political divisions of the United States\"\r\n",
      "0.00013003778509485187\t\"14963657 - population density\"\r\n",
      "0.00012841221637639494\t\"2578813 - Catholic Church\"\r\n",
      "0.0001282046223090247\t\"2826544 - Chordate\"\r\n",
      "0.00012732350077934083\t\"1575979 - BBC\"\r\n",
      "0.00012715357035669744\t\"1813634 - Belgium\"\r\n",
      "0.00012410867750278315\t\"2778099 - Chicago\"\r\n",
      "0.00012093630817512696\t\"13853369 - Washington, D.C.\"\r\n",
      "0.00012024237237818629\t\"9924814 - Pakistan\"\r\n",
      "0.0001157792173901934\t\"4568647 - Finland\"\r\n",
      "0.00011450767260764365\t\"12785678 - The Guardian\"\r\n",
      "0.00011447345057904762\t\"7467127 - Latin\"\r\n",
      "0.00011430180463041426\t\"9742161 - Ontario\"\r\n",
      "0.00011359377946447987\t\"3328327 - Czech Republic\"\r\n",
      "0.0001132654264571577\t\"10246542 - Philippines\"\r\n",
      "0.00011323587651842911\t\"3591832 - Denmark\"\r\n",
      "0.00011319290359763953\t\"5274313 - Greece\"\r\n",
      "0.00011291110378232016\t\"14727077 - genus\"\r\n",
      "0.00011241645992615994\t\"14709489 - football (soccer)\"\r\n",
      "0.00011218707479056177\t\"5908108 - Hungary\"\r\n",
      "0.00011212013199291848\t\"3973000 - Eastern European Time\"\r\n"
     ]
    }
   ],
   "source": [
    "#!python RunPageRank.py -g 'hdfs:///user/leiyang/all-pages-indexed-out.txt' -j 0.15 -i 50 \\\n",
    "#-d 'hdfs:///user/leiyang/indices.txt' -s '0'\n",
    "!cat ./data/wiki_50_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Comments\n",
    "- the top 100 node are identical except for two of them swapped their place (\"9391762 - New York\" and \"2614578 - Central European Summer Time\")\n",
    "- convergence is pretty fast for this graph with wiki nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "###HW 9.4: Topic-specific PageRank implementation using MRJob\n",
    "\n",
    "Modify your PageRank implementation to produce a topic specific PageRank implementation,\n",
    "as described in:\n",
    "\n",
    "http://www-cs-students.stanford.edu/~taherh/papers/topic-sensitive-pagerank.pdf\n",
    "\n",
    "Note in this article that there is a special caveat to ensure that the transition matrix is irreducible.\n",
    "This caveat lies in footnote 3 on page 3:\n",
    "\n",
    "\tA minor caveat: to ensure that M is irreducible when p\n",
    "\tcontains any 0 entries, nodes not reachable from nonzero\n",
    "\tnodes in p should be removed. In practice this is not problematic.\n",
    "\n",
    "and must be adhered to for convergence to be guaranteed.\n",
    "\n",
    "Run topic specific PageRank on the following randomly generated network of 100 nodes:\n",
    "\n",
    "s3://ucb-mids-mls-networks/randNet.txt (also available on Dropbox)\n",
    "\n",
    "which are organized into ten topics, as described in the file:\n",
    "\n",
    "s3://ucb-mids-mls-networks/randNet_topics.txt  (also available on Dropbox)\n",
    "\n",
    "Since there are 10 topics, your result should be 11 PageRank vectors\n",
    "(one for the vanilla PageRank implementation in 9.1, and one for each topic\n",
    "with the topic specific implementation). Print out the **top ten** ranking nodes \n",
    "and their topics for each of the 11 versions, and comment on your result. \n",
    "Assume a teleportation factor of **0.15** in all your analyses.\n",
    "\n",
    "One final and important comment here:  please consider the \n",
    "requirements for irreducibility with topic-specific PageRank.\n",
    "In particular, the literature ensures irreducibility by requiring that\n",
    "nodes not reachable from in-topic nodes be removed from the network.\n",
    "\n",
    "This is not a small task, especially as it it must be performed\n",
    "separately for each of the (10) topics.\n",
    "\n",
    "So, instead of using this method for irreducibility, \n",
    "please comment on why the literature's method is difficult to implement,\n",
    "and what what extra computation it will require.\n",
    "Then for your code, please use the alternative, \n",
    "non-uniform damping vector:\n",
    "\n",
    "$v_{ji} = \\frac{\\beta}{|T_j|}$; if node $i$ lies in topic $T_j$\n",
    "\n",
    "$v_{ji} = \\frac{1-\\beta}{N - |Tj|}$; if node $i$ lies outside of topic $T_j$\n",
    "\n",
    "for $\\beta$ in (0,1) close to 1. \n",
    "\n",
    "With this approach, you will not have to delete any nodes.\n",
    "If $\\beta > 0.5$, PageRank is topic-sensitive, \n",
    "and if $\\beta < 0.5$, the PageRank is anti-topic-sensitive. \n",
    "For any value of $\\beta$ irreducibility should hold,\n",
    "so please try $\\beta=0.99$, and perhaps some other values locally,\n",
    "on the smaller networks.\n",
    "\n",
    "\n",
    "### Implementation Notes - two changes on the vanilla PageRank implementation:\n",
    "\n",
    "####1. during mass distribution step, in _PageRankIter_ job:\n",
    " - maintain an array of ranks for each topic\n",
    " - distribute and accumulate rank number separately for each rank number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankIter_T.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankIter_T.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class PageRankIter_T(MRJob):\n",
    "    DEFAULT_PROTOCOL = 'json'\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(PageRankIter_T, self).configure_options()        \n",
    "        self.add_passthrough_option(\n",
    "            '--i', dest='init', default='0', type='int',\n",
    "            help='i: run initialization iteration (default 0)') \n",
    "        self.add_passthrough_option(\n",
    "            '--n', dest='n_topic', default='0', type='int',\n",
    "            help='n: number of topics (default 0)') \n",
    "\n",
    "    def mapper_job_init(self, _, line):        \n",
    "        # parse line\n",
    "        nid, adj = line.strip().split('\\t', 1)\n",
    "        nid = nid.strip('\"')\n",
    "        cmd = 'adj = %s' %adj\n",
    "        exec cmd\n",
    "        # initialize node struct        \n",
    "        node = {'a':adj.keys(), 'p':[0]*(self.options.n_topic + 1)}\n",
    "        # vanillar PageRank and topic sensitive PageRank\n",
    "        rankMass = [1.0 / len(adj)] * (self.options.n_topic + 1)\n",
    "        # emit node\n",
    "        yield nid, node\n",
    "        # emit pageRank mass        \n",
    "        for m in node['a']:\n",
    "            yield m, rankMass\n",
    "            \n",
    "    def mapper_job_iter(self, _, line):             \n",
    "        # parse line\n",
    "        nid, node = line.strip().split('\\t', 1)\n",
    "        nid = nid.strip('\"')\n",
    "        cmd = 'node = %s' %node\n",
    "        exec cmd\n",
    "        # distribute rank mass  \n",
    "        n_adj = len(node['a'])\n",
    "        if n_adj > 0:\n",
    "            rankMass = [x / n_adj for x in node['p']]\n",
    "            # emit pageRank mass        \n",
    "            for m in node['a']:\n",
    "                yield m, rankMass\n",
    "        else:\n",
    "            # track dangling mass for each topic with counters\n",
    "            for i in range(self.options.n_topic+1):\n",
    "                self.increment_counter('wiki_dangling_mass', 'topic_%d' %i, int(node['p'][i]*1e10))\n",
    "        # reset pageRank and emit node\n",
    "        node['p'] = [0]*(self.options.n_topic+1)\n",
    "        yield nid, node\n",
    "    \n",
    "    def debug(self):\n",
    "        de = 'bug'\n",
    "                \n",
    "    # write a separate combiner ensure the integrity of the graph topology\n",
    "    # no additional node object will be generated\n",
    "    def combiner(self, nid, value):             \n",
    "        rankMass, node = [0]*(self.options.n_topic+1), None        \n",
    "        # loop through all arrivals\n",
    "        for v in value:            \n",
    "            if isinstance(v, list):\n",
    "                rankMass = [a+b for a,b in zip(rankMass, v)]                \n",
    "            else:\n",
    "                node = v            \n",
    "        # emit accumulative mass for nid       \n",
    "        if node:\n",
    "            node['p'] = [a+b for a,b in zip(rankMass, node['p'])]\n",
    "            yield nid, node\n",
    "        else:\n",
    "            yield nid, rankMass\n",
    "    \n",
    "    # reducer for initialization pass --> need to handle dangling nodes\n",
    "    def reducer_job_init(self, nid, value):      \n",
    "        # increase counter for node count\n",
    "        self.increment_counter('wiki_node_count', 'nodes', 1)\n",
    "        rankMass, node = [0]*(self.options.n_topic+1), None\n",
    "        # loop through all arrivals\n",
    "        for v in value:            \n",
    "            if isinstance(v, list):\n",
    "                rankMass = [a+b for a,b in zip(rankMass, v)]         \n",
    "            else:\n",
    "                node = v\n",
    "        # handle dangling node, create node struct and add missing mass\n",
    "        if not node:            \n",
    "            node = {'a':[], 'p':rankMass}   \n",
    "            for i in range(self.options.n_topic+1):\n",
    "                self.increment_counter('wiki_dangling_mass', 'mass_%d' %i, int(1e10))\n",
    "        else:\n",
    "            node['p'] = [a+b for a,b in zip(rankMass, node['p'])]\n",
    "        # emit for next iteration\n",
    "        yield nid, node\n",
    "        \n",
    "    # reducer for regular pass --> all nodes has structure available\n",
    "    def reducer_job_iter(self, nid, value):              \n",
    "        rankMass, node = [0]*(self.options.n_topic+1), None\n",
    "        # loop through all arrivals\n",
    "        for v in value:            \n",
    "            if isinstance(v, list):\n",
    "                rankMass = [a+b for a,b in zip(rankMass, v)]        \n",
    "            else:\n",
    "                node = v\n",
    "        # update pageRank\n",
    "        node['p'] = [a+b for a,b in zip(rankMass, node['p'])]            \n",
    "        # emit for next iteration\n",
    "        yield nid, node\n",
    "\n",
    "    def steps(self):\n",
    "        jc = {\n",
    "            'mapreduce.job.maps': '2',\n",
    "            'mapreduce.job.reduces': '2',\n",
    "        }\n",
    "        return [MRStep(mapper=self.mapper_job_init if self.options.init else self.mapper_job_iter                       \n",
    "                       , combiner=self.combiner                       \n",
    "                       , reducer=self.reducer_job_init if self.options.init else self.reducer_job_iter\n",
    "                       , jobconf = jc\n",
    "                      )\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PageRankIter_T.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### unit test #####\n",
    "#!python PageRankIter_T.py ./data/randNet.txt --i 1 --n 10 -r 'inline' > test.t\n",
    "#!python PageRankIter_T.py ./data/PageRank-test.txt --i 1 --n 10 -r 'hadoop' > test.t\n",
    "#!python PageRankIter_T.py test.t --i 0 --n 2 -r 'inline' > test2.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####2. during mass redistribution/adjustment step, in _PageRankDist_ job:\n",
    " \n",
    "$$p'=\\alpha\\big(\\frac{1}{|G|}\\big)+(1-\\alpha)\\big(\\frac{m}{|G|}+p\\big),\\quad |G|=N$$\n",
    "\n",
    "  - instead of all nodes receiving $\\frac{1}{N}$ uniformly in the teleportation term, nodes belong to topic $T_j$ will receive $\\frac{\\beta}{|T_j|}$, and others receive $\\frac{1-\\beta}{N - |Tj|}$\n",
    "  \n",
    "\n",
    "  - intuitively with $\\beta=0.99$, the teleportation will transition to nodes in $T_j$ with a $99\\%$ probability, while only $1\\%$ chance to nodes outside of $T_j$\n",
    "\n",
    "\n",
    "  - because $\\beta$ and $|T_j|$ are both known, we will evaluate $v_{ji}$ before hand and load all $10$ of them in _mapper_init_ phase, and apply it in the mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankDist_T.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankDist_T.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "class PageRankDist_T(MRJob):\n",
    "    DEFAULT_PROTOCOL = 'json'\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(PageRankDist_T, self).configure_options()        \n",
    "        self.add_passthrough_option(\n",
    "            '--s', dest='size', default=0, type='int',\n",
    "            help='size: node number (default 0)')    \n",
    "        self.add_passthrough_option(\n",
    "            '--j', dest='alpha', default=0.15, type='float',\n",
    "            help='jump: teleport factor (default 0.15)') \n",
    "        self.add_passthrough_option(\n",
    "            '--b', dest='beta', default=0.99, type='float',\n",
    "            help='beta: topic bias factor (default 0.99)') \n",
    "        self.add_passthrough_option(\n",
    "            '--m', dest='m', default='', type='str',\n",
    "            help='m: rank mass from dangling nodes') \n",
    "        self.add_passthrough_option(\n",
    "            '--w', dest='wiki', default=0, type='int',\n",
    "            help='w: if it is wiki data (default 1)') \n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # load topic file and count\n",
    "        T_j, self.T_index = {}, {}\n",
    "        cat = Popen(['cat', 'randNet_topics.txt'], stdout=PIPE)\n",
    "        for line in cat.stdout:\n",
    "            nid, topic = line.strip().split('\\t')\n",
    "            self.T_index[nid] = topic\n",
    "            T_j[topic] = 1 if topic not in T_j else (T_j[topic]+1)\n",
    "            \n",
    "        # prepare adjustment factors\n",
    "        self.damping = 1 - self.options.alpha        \n",
    "        cmd = 'm = %s' %self.options.m\n",
    "        exec cmd\n",
    "        # assuming here -m is specified with a list syntax string\n",
    "        self.p_dangling = [1.0*x / self.options.size for x in m]\n",
    "        # for each topic, get topic bias\n",
    "        self.v_ij = [[1, 1]]*(len(T_j)+1)\n",
    "        N, b = self.options.size, self.options.beta\n",
    "        for t in T_j:\n",
    "            self.v_ij[int(t)] = [(1-b)*N/(N-T_j[t]), b*N/T_j[t]]\n",
    "                \n",
    "            \n",
    "    def mapper(self, _, line):             \n",
    "        # parse line\n",
    "        nid, node = line.strip().split('\\t', 1)\n",
    "        nid = nid.strip('\"')\n",
    "        cmd = 'node = %s' %node\n",
    "        exec cmd\n",
    "        # get final pageRank        \n",
    "        for i in range(len(self.v_ij)):\n",
    "            vij = self.v_ij[i][i==int(self.T_index[nid])]\n",
    "            node['p'][i] = (self.p_dangling[i]+node['p'][i])*self.damping + self.options.alpha*vij\n",
    "        yield nid, node\n",
    "\n",
    "    def steps(self):\n",
    "        jc = {\n",
    "            'mapreduce.job.maps': '2',           \n",
    "        }\n",
    "        return [MRStep(mapper_init=self.mapper_init\n",
    "                       , mapper=self.mapper                    \n",
    "                       , jobconf = jc\n",
    "                      )\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PageRankDist_T.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### unit test #####\n",
    "# no dangling nodes for randNet data\n",
    "#!python PageRankDist_T.py test.t --m '[1]*11' --s '100' --file './data/randNet_topics.txt'  -r 'hadoop' > test2.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PageRankSort job \n",
    "- emit (vector_ID, pageRank)~topic_id as key~value pair\n",
    "- partition on vector_ID, secondary sort (-k2,2nr) on pageRank \n",
    "- print out top 10 for each vector_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankSort_T.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankSort_T.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "class PageRankSort_T(MRJob):\n",
    "    DEFAULT_PROTOCOL = 'json'\n",
    "    PARTITIONER = 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # load topic file and count\n",
    "        self.T_index = {}\n",
    "        cat = Popen(['cat', 'randNet_topics.txt'], stdout=PIPE)\n",
    "        for line in cat.stdout:\n",
    "            nid, topic = line.strip().split('\\t')\n",
    "            self.T_index[nid] = topic                \n",
    "            \n",
    "    def mapper(self, _, line):             \n",
    "        # parse line\n",
    "        nid, node = line.strip().split('\\t', 1)\n",
    "        nid = nid.strip('\"')\n",
    "        cmd = 'node = %s' %node\n",
    "        exec cmd\n",
    "        # emit (vector_ID, pageRank)~topic_id\n",
    "        for i in range(len(node['p'])):\n",
    "            yield (i, node['p'][i]), self.T_index[nid]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def reducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield key, v\n",
    "        \n",
    "    \n",
    "    def steps(self):\n",
    "        jc = {\n",
    "            'mapreduce.job.maps': '3',\n",
    "            'mapreduce.job.reduces': '3',\n",
    "            #'partitioner': 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner', # doesn't work here, must be set at top\n",
    "            'mapreduce.partition.keypartitioner.options': '-k1,1',             \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1 -k2,2nr',            \n",
    "            'stream.num.map.output.key.fields': '2',\n",
    "            'mapreduce.map.output.key.field.separator': ' ',\n",
    "            'stream.map.output.field.separator': ' ',   \n",
    "        }\n",
    "        return [MRStep(mapper_init=self.mapper_init\n",
    "                       , mapper=self.mapper       \n",
    "                       , reducer=self.reducer\n",
    "                       , jobconf = jc\n",
    "                      )\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PageRankSort_T.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/leiyang/.mrjob.conf\n",
      "Got unexpected opts from /Users/leiyang/.mrjob.conf: no_output\n",
      "creating tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/PageRankSort_T.leiyang.20160315.023813.337114\n",
      "writing wrapper script to /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/PageRankSort_T.leiyang.20160315.023813.337114/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/leiyang/tmp/mrjob/PageRankSort_T.leiyang.20160315.023813.337114/files/\n",
      "HADOOP: packageJobJar: [/var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/hadoop-unjar6498154444913894063/] [] /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/streamjob7794783564353224717.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/leiyang/tmp/mrjob/PageRankSort_T.leiyang.20160315.023813.337114/output\n",
      "removing tmp directory /var/folders/tx/5ldq67q511q8wqwqkvptnxd00000gn/T/PageRankSort_T.leiyang.20160315.023813.337114\n",
      "deleting hdfs:///user/leiyang/tmp/mrjob/PageRankSort_T.leiyang.20160315.023813.337114 from HDFS\n"
     ]
    }
   ],
   "source": [
    "##### unit test #####\n",
    "!python PageRankSort_T.py 'randNet_pr' --file './data/randNet_topics.txt' -r 'hadoop' > test.sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Driver for topic-sensitive-pagerank\n",
    "- we need to retrieve a group of counters, which represent loss mass for each topic. \n",
    "- in this implementation, the counter group name is **wiki_dangling_mass**, and counter names are **mass_[i]**, with mass_0 for vanilla pageRank, and mass_1 for topic 1 etc.\n",
    "- the loss values are passed as list syntax string to the job, which will transfer it to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting RunPageRank_T.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile RunPageRank_T.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from PageRankIter_T import PageRankIter_T\n",
    "from PageRankDist_T import PageRankDist_T\n",
    "from PageRankSort import PageRankSort\n",
    "from PageRankJoin import PageRankJoin\n",
    "from helper import getCounter, getCounters\n",
    "from subprocess import call, check_output\n",
    "from time import time\n",
    "import sys, getopt, datetime, os\n",
    "\n",
    "# parse parameter\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(sys.argv[1:], \"hg:j:i:d:s:\")\n",
    "    except getopt.GetoptError:\n",
    "        print 'RunBFS.py -g <graph> -j <jump> -i <iteration> -d <index> -s <size>'\n",
    "        sys.exit(2)\n",
    "    if len(opts) != 5:\n",
    "        print 'RunBFS.py -g <graph> -j <jump> -i <iteration> -d <index>'\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print 'RunBFS.py -g <graph> -j <jump> -i <iteration> -d <index>'\n",
    "            sys.exit(2)\n",
    "        elif opt == '-g':\n",
    "            graph = arg\n",
    "        elif opt == '-j':\n",
    "            jump = arg\n",
    "        elif opt == '-i':            \n",
    "            n_iter = arg\n",
    "        elif opt == '-d':\n",
    "            index = arg\n",
    "        elif opt == '-s':\n",
    "            n_node = arg\n",
    "        \n",
    "start = time()\n",
    "FNULL = open(os.devnull, 'w')\n",
    "n_iter = int(n_iter)\n",
    "doJoin = index!='NULL'\n",
    "doInit = n_node=='0'\n",
    "host = 'localhost'\n",
    "\n",
    "print '%s: %s topic sensitive PageRanking on \\'%s\\' for %d iterations with damping factor %.2f ...' %(str(datetime.datetime.now()),\n",
    "          'start' if doInit else 'continue', graph[graph.rfind('/')+1:], n_iter, 1-float(jump))\n",
    "\n",
    "if doInit:\n",
    "    # clear directory\n",
    "    print str(datetime.datetime.now()) + ': clearing directory ...'\n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/in'], stdout=FNULL)\n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/out'], stdout=FNULL)\n",
    "    \n",
    "    # creat initialization job    \n",
    "    init_job = PageRankIter_T(args=[graph, '--i', '1', '--n', '10', '-r', 'hadoop', \n",
    "                                    '--output-dir', 'hdfs:///user/leiyang/out'])\n",
    "\n",
    "    # run initialization job\n",
    "    print str(datetime.datetime.now()) + ': running iteration 1 ...'\n",
    "    with init_job.make_runner() as runner:    \n",
    "        runner.run()\n",
    "\n",
    "    # checking counters\n",
    "    n_node = getCounter('wiki_node_count', 'nodes', host)    \n",
    "    loss = getCounters('wiki_dangling_mass', host)\n",
    "    loss_array = ['0']*11\n",
    "    for k in loss:\n",
    "        i = int(k.split('_')[1])\n",
    "        loss_array[i] = str(loss[k]/1e10)\n",
    "    print '%s: initialization complete: %d nodes!' %(str(datetime.datetime.now()), n_node)\n",
    "\n",
    "    #!python PageRankDist_T.py test.t --m '[1]*11' --s '100' --file './data/randNet_topics.txt'  -r 'hadoop' > test2.t\n",
    "    # run redistribution job\n",
    "    call(['hdfs', 'dfs', '-mv', '/user/leiyang/out', '/user/leiyang/in'])\n",
    "    loss_param = '[%s]' %(','.join(['0']*11) if len(loss)==0 else ','.join(loss_array))\n",
    "    dist_job = PageRankDist_T(args=['hdfs:///user/leiyang/in/part*', '--s', str(n_node), '--m', loss_param,\n",
    "                                    '--file', 'hdfs:///user/leiyang/randNet_topics.txt',\n",
    "                                    '-r', 'hadoop', '--output-dir', 'hdfs:///user/leiyang/out'])\n",
    "    print str(datetime.datetime.now()) + ': distributing loss mass ...'\n",
    "    with dist_job.make_runner() as runner:    \n",
    "        runner.run()\n",
    "\n",
    "# move results for next iteration\n",
    "call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/in'], stdout=FNULL)\n",
    "call(['hdfs', 'dfs', '-mv', '/user/leiyang/out', '/user/leiyang/in'])\n",
    "\n",
    "# create iteration job\n",
    "iter_job = PageRankIter_T(args=['hdfs:///user/leiyang/in/part*', '--i', '0', '--n', '10',\n",
    "                              '-r', 'hadoop', '--output-dir', 'hdfs:///user/leiyang/out'])\n",
    "\n",
    "# run pageRank iteratively\n",
    "i = 2 if doInit else 1\n",
    "while(1):\n",
    "    print str(datetime.datetime.now()) + ': running iteration %d ...' %i\n",
    "    with iter_job.make_runner() as runner:        \n",
    "        runner.run()\n",
    "    \n",
    "    # check counters for topic loss mass\n",
    "    loss = getCounters('wiki_dangling_mass', host)\n",
    "    loss_array = ['0']*11\n",
    "    for k in loss:\n",
    "        i = int(k.split('_')[1])\n",
    "        loss_array[i] = str(loss[k]/1e10)\n",
    "    \n",
    "    # move results for next iteration\n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/in'], stdout=FNULL)\n",
    "    call(['hdfs', 'dfs', '-mv', '/user/leiyang/out', '/user/leiyang/in'])\n",
    "        \n",
    "    # run redistribution job\n",
    "    loss_param = '[%s]' %(','.join(['0']*11) if len(loss)==0 else ','.join(loss_array))\n",
    "    dist_job = PageRankDist_T(args=['hdfs:///user/leiyang/in/part*', '--s', str(n_node), '--m', loss_param,\n",
    "                                    '--file', 'hdfs:///user/leiyang/randNet_topics.txt',\n",
    "                                    '-r', 'hadoop', '--output-dir', 'hdfs:///user/leiyang/out'])\n",
    "    \n",
    "    print str(datetime.datetime.now()) + ': distributing loss mass ...'\n",
    "    with dist_job.make_runner() as runner:    \n",
    "        runner.run()\n",
    "    \n",
    "    if i == n_iter:\n",
    "        break\n",
    "    \n",
    "    # if more iteration needed\n",
    "    i += 1    \n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/in'], stdout=FNULL)\n",
    "    call(['hdfs', 'dfs', '-mv', '/user/leiyang/out', '/user/leiyang/in'], stdout=FNULL)\n",
    "\n",
    "# run sort job\n",
    "#print str(datetime.datetime.now()) + ': sorting PageRank ...'\n",
    "#call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/rank'], stdout=FNULL)\n",
    "#sort_job = PageRankSort(args=['hdfs:///user/leiyang/out/part*', '--s', str(n_node), '--n', '100',\n",
    "#                              '-r', 'hadoop', '--output-dir', 'hdfs:///user/leiyang/rank'])\n",
    "#with sort_job.make_runner() as runner:    \n",
    "#    runner.run()\n",
    "    \n",
    "# run join job\n",
    "if doJoin:\n",
    "    print str(datetime.datetime.now()) + ': joining PageRank with index ...'\n",
    "    call(['hdfs', 'dfs', '-rm', '-r', '/user/leiyang/join'], stdout=FNULL)\n",
    "    join_job = PageRankJoin(args=[index, '-r', 'hadoop', '--file', 'hdfs:///user/leiyang/rank/part-00000', \n",
    "                                  '--output-dir', 'hdfs:///user/leiyang/join'])\n",
    "    with join_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "print \"%s: PageRank job completes in %.1f minutes!\\n\" %(str(datetime.datetime.now()), (time()-start)/60.0)\n",
    "\n",
    "# TODO\n",
    "# copy results to S3: \n",
    "#call(['hdfs', 'dfs', '-cat', '/user/leiyang/join/p*' if doJoin else '/user/leiyang/rank/p*', '>', 'results_wiki_50'])\n",
    "#call(['aws', 's3', 'cp', 'results_wiki_50', 's3://w261.data/HW9/results_wiki_50', '--region', 'us-west-2'])\n",
    "# terminate cluster: - aws emr terminate-clusters --cluster-ids j-CT2NH23KIIBL\n",
    "#print \"%s: results copied to S3, shutting down cluster ...\" %(str(datetime.datetime.now()))\n",
    "#call(['aws', 'emr', 'terminate-clusters', '--cluster-ids', 'j-CT2NH23KIIBL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Run topic-sensitive-PageRank on randNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-14 21:52:44.987294: start topic sensitive PageRanking on 'randNet.txt' for 10 iterations with damping factor 0.85 ...\n",
      "2016-03-14 21:52:44.987357: clearing directory ...\n",
      "2016-03-14 21:52:48.269763: running iteration 1 ...\n",
      "No handlers could be found for logger \"mrjob.conf\"\n",
      "2016-03-14 21:53:19.347179: initialization complete: 100 nodes!\n",
      "2016-03-14 21:53:20.901768: distributing loss mass ...\n",
      "2016-03-14 21:53:47.742804: running iteration 2 ...\n",
      "2016-03-14 21:54:21.963305: distributing loss mass ...\n",
      "2016-03-14 21:54:48.897135: running iteration 3 ...\n",
      "2016-03-14 21:55:22.801896: distributing loss mass ...\n",
      "2016-03-14 21:55:49.033106: running iteration 4 ...\n",
      "2016-03-14 21:56:22.505089: distributing loss mass ...\n",
      "2016-03-14 21:56:48.714427: running iteration 5 ...\n",
      "2016-03-14 21:57:23.632246: distributing loss mass ...\n",
      "2016-03-14 21:57:51.483645: running iteration 6 ...\n",
      "2016-03-14 21:58:25.252929: distributing loss mass ...\n",
      "2016-03-14 21:58:51.375450: running iteration 7 ...\n",
      "2016-03-14 21:59:25.065766: distributing loss mass ...\n",
      "2016-03-14 21:59:51.428620: running iteration 8 ...\n",
      "2016-03-14 22:00:24.914882: distributing loss mass ...\n",
      "2016-03-14 22:00:51.355740: running iteration 9 ...\n",
      "2016-03-14 22:01:25.462349: distributing loss mass ...\n",
      "2016-03-14 22:01:51.994726: running iteration 10 ...\n",
      "2016-03-14 22:02:24.638326: distributing loss mass ...\n",
      "2016-03-14 22:02:48.004800: PageRank job completes in 10.1 minutes!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python RunPageRank_T.py -g 'hdfs:///user/leiyang/randNet.txt' -j 0.15 -i 10 -d 'NULL' -s '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 9.5: Applying topic-specific PageRank to Wikipedia\n",
    "\n",
    "Here you will apply your topic-specific PageRank implementation to Wikipedia,\n",
    "defining topics (very arbitrarily) for each page by the length (number of characters) of the name of the article mod 10,\n",
    "so that there are 10 topics. Once again, print out the top ten ranking nodes \n",
    "and their topics for each of the 11 versions, and comment on your result.\n",
    "Assume a teleportation factor of 0.15 in all your analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### unit test #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 9.6: TextRank (OPTIONAL)\n",
    "\n",
    "What is TextRank. Describe the main steps in the algorithm. Why does TextRank work?\n",
    "Implement TextRank in MrJob for keyword phrases (not just unigrams) extraction using co-occurrence based similarity measure with with sizes of N = 2 and 3. And evaluate your code using the following example using precision, recall, and FBeta (Beta=1):\n",
    "\n",
    "\"Compatibility of systems of linear constraints over the set of natural numbers\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict \n",
    "inequations, and nonstrict inequations are considered. Upper bounds for\n",
    "components of a minimal set of solutions and algorithms of construction of \n",
    "minimal generating sets of solutions for all types of systems are given. \n",
    "These criteria and the corresponding algorithms for constructing a minimal \n",
    "supporting set of solutions can be used in solving all the considered types of \n",
    "systems and systems of mixed types.\" \n",
    "\n",
    "The extracted keywords should in the following set:\n",
    "\n",
    "linear constraints, linear diophantine equations, natural numbers, non-strict inequations, strict inequations, upper bounds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### unit test #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MrJob to check if traverse completes for unweighted graph\n",
    "- as soon as the destination is reached, traverse completes, shortest path found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### unit test #####\n",
    "#!python isDestinationReached.py test1 --destination 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "stopping historyserver\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ stop historyserver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###start yarn, hdfs, and job history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-resourcemanager-Leis-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-leiyang-nodemanager-Leis-MacBook-Pro.local.out\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-namenode-Leis-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-datanode-Leis-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-leiyang-secondarynamenode-Leis-MacBook-Pro.local.out\n",
      "starting historyserver, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/mapred-leiyang-historyserver-Leis-MacBook-Pro.local.out\n",
      "16/03/14 20:56:22 INFO hs.JobHistoryServer: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting JobHistoryServer\n",
      "STARTUP_MSG:   host = leis-macbook-pro.local/192.168.0.12\n",
      "STARTUP_MSG:   args = []\n",
      "STARTUP_MSG:   version = 2.7.1\n",
      "STARTUP_MSG:   classpath = /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop/:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/activation-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/contrib/capacity-scheduler/*.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/Cellar/hadoop/2.7.1/libexec/modules/*.jar\n",
      "STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z\n",
      "STARTUP_MSG:   java = 1.7.0_79\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2*/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2*/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2*/libexec/etc/hadoop/ start historyserver "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
